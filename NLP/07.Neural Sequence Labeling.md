# Neural Sequence Labeling

### Indice dei contenuti

* [Neural Sequence Labeling](#neural-sequence-labeling)
    * [Indice dei contenuti](#indice-dei-contenuti)
    * [Sequence processing](#sequence-processing)
    * [MEMM: Maximum Entropy Markov Model](#memm-maximum-entropy-markov-model)
        * [Label bias](#label-bias)
    * [Conditional Random Fields](#conditional-random-fields)
    * [BiRNN: Bidirectional Recurrent Neural Networks](#birnn-bidirectional-recurrent-neural-networks)
    * [LSTM: Long Short Term Memory](#lstm-long-short-term-memory)
        * [Gate LSTM](#gate-lstm)
    * [GRU: Gated Recurrent Unit](#gru-gated-recurrent-unit)
    * [Padding](#padding)
    * [Self attention](#self-attention)
        * [Attention (classica) vs Self-Attention](#attention-classica-vs-self-attention)

---

## Sequence processing

Il termine "Sequence Processing" si riferisce a un'ampia categoria di task e tecniche utilizzate nell'ambito
dell'elaborazione delle sequenze, che comprendono sequenze di dati, come sequenze di parole in un testo, sequenze
temporali in dati temporali o sequenze di simboli in input sequenziali.

Ecco alcune delle principali definizioni e concetti correlati al Sequence Processing:

1. **Sequenze di Dati**: Una sequenza è un insieme di dati o elementi ordinati. Ad esempio, una frase è una sequenza di
   parole, una serie temporale è una sequenza di punti dati registrati nel tempo, e così via.

2. **Sequence Processing Tasks**: Queste sono attività che implicano l'elaborazione o l'analisi di sequenze di dati.
   Alcuni esempi includono la traduzione automatica (traduzione da una lingua all'altra), il riconoscimento di entità
   nominata (identificazione di nomi propri in un testo), il riconoscimento del parlato (trascrizione di discorsi orali
   in testo), la generazione di testi (creazione di testi coerenti e significativi) e molto altro.

3. **Modelli di Sequence Processing**: Questi sono algoritmi e reti neurali progettati per gestire dati sequenziali. Tra
   i modelli più comuni ci sono le Recurrent Neural Networks (RNN), le Long Short-Term Memory (LSTM), le Gated Recurrent
   Unit (GRU), i Transformer e molti altri. Questi modelli sono adatti per catturare le dipendenze e le strutture
   all'interno delle sequenze.

4. **Apprendimento Supervisionato**: Nella maggior parte dei task di Sequence Processing, si utilizza un approccio di
   apprendimento supervisionato, in cui il modello è addestrato su un dataset etichettato che contiene esempi di input e
   le relative uscite attese. Il modello apprende a fare previsioni basate su questi esempi.

5. **Apprendimento Non Supervisionato**: Tuttavia, esistono anche approcci di apprendimento non supervisionato per
   Sequence Processing, come il clustering sequenziale o la ricerca di pattern nelle sequenze senza l'uso di etichette.

In sintesi, Sequence Processing si riferisce all'elaborazione, all'analisi e alla manipolazione di dati organizzati in
sequenze ordinate. Questa è un'area fondamentale nell'ambito dell'elaborazione del linguaggio naturale, del
riconoscimento del parlato, dell'analisi temporale delle serie storiche e di molte altre applicazioni in cui è
necessario considerare l'ordine dei dati per comprenderne il significato o effettuare previsioni.

---

## MEMM: Maximum Entropy Markov Model

Il MEMM, o Maximum Entropy Markov Model, è un tipo di modello statistico utilizzato nell'ambito dell'elaborazione del
linguaggio naturale (NLP) e in altri campi per la modellazione sequenziale. L'addestramento di un MEMM è il processo
attraverso il quale il modello apprende dai dati di addestramento per fare previsioni su sequenze di dati in futuro.
Ecco come funziona il training di un MEMM:

1. **Dati di Addestramento**:
    - Il processo di addestramento inizia con un insieme di dati di addestramento, che sono sequenze di eventi o dati
      etichettati. Ad esempio, in NLP, potrebbe essere un insieme di frasi etichettate con parti del discorso o un
      insieme di parole etichettate con le loro traduzioni in una lingua diversa.

2. **Caratteristiche**:
    - Per ciascun evento o passo nella sequenza, vengono estratte delle caratteristiche (features) rilevanti dal
      contesto. Queste caratteristiche possono includere informazioni sulle parole circostanti, le posizioni nella
      sequenza, le etichette precedentemente assegnate, ecc.

3. **Definizione del Modello**:
    - Si definisce il modello MEMM, che è una forma specifica di modello probabilistico condizionato su una sequenza. Il
      modello assegna una probabilità condizionata a ciascun evento nella sequenza in base alle caratteristiche
      estratte.

4. **Addestramento**:
    - Durante l'addestramento, il modello MEMM impara dai dati di addestramento. L'obiettivo è massimizzare la
      probabilità dei dati di addestramento sotto il modello. Questo significa che il modello cerca di assegnare
      probabilità più alte agli eventi effettivamente osservati nei dati di addestramento.
    - L'addestramento di un MEMM coinvolge tipicamente l'ottimizzazione di parametri del modello in modo da massimizzare
      la verosimiglianza dei dati di addestramento. Ciò può essere fatto utilizzando algoritmi di ottimizzazione come il
      gradiente stocastico (SGD) o il metodo di Newton.

5. **Valutazione**:
    - Dopo l'addestramento, il modello MEMM viene valutato utilizzando dati di test o dati di validazione per misurare
      le sue prestazioni. Questo aiuta a determinare quanto bene il modello generalizza ai dati che non ha visto durante
      l'addestramento.

6. **Utilizzo**:
    - Una volta addestrato e valutato, il modello MEMM può essere utilizzato per fare previsioni su nuove sequenze di
      dati. Ad esempio, può essere utilizzato per effettuare il riconoscimento delle entità nominati o altre attività di
      etichettatura sequenziale.

In sintesi, il training di un MEMM coinvolge l'addestramento di un modello statistico condizionato sulle sequenze di
dati. Il modello impara dai dati di addestramento e viene utilizzato per fare previsioni su nuove sequenze in base alle
caratteristiche estratte dal contesto. Questo è un approccio comune in NLP e in altre applicazioni di modellazione
sequenziale.

### Label bias

Il "label bias" (o "bias dell'etichetta") è un concetto importante da considerare quando si lavora con modelli di
etichettatura sequenziale come il Maximum Entropy Markov Model (MEMM). Si riferisce a una tendenza intrinseca del MEMM a
preferire etichette più comuni o a mostrare una certa inclinazione verso certe etichette nelle sequenze.

Ecco come il label bias influisce sul MEMM:

1. **Influenza sulla Distribuzione delle Etichette**: A causa del label bias, il MEMM può tendere a generare sequenze di
   etichette che riflettono le etichette più comuni nei dati di addestramento. Ad esempio, se durante l'addestramento è
   più comune vedere una parola etichettata come "sostantivo" piuttosto che "verbo," il modello MEMM potrebbe essere
   incline a etichettare più parole come "sostantivo."

2. **Difficoltà nel Gestire Etichette Rare**: Il MEMM può avere difficoltà a gestire etichette rare o poco comuni,
   poiché ha una tendenza a preferire etichette più frequenti. Ad esempio, se un'etichetta come "onomatopea" appare
   raramente nei dati di addestramento, il MEMM potrebbe avere difficoltà a etichettare correttamente le onomatopee
   nelle nuove sequenze.

3. **Soluzioni al Label Bias**: Per mitigare il label bias, è possibile adottare diverse strategie durante
   l'addestramento del MEMM, come pesare le etichette in base alla loro frequenza per darne maggiore importanza alle
   etichette rare o utilizzare tecniche di apprendimento attive per acquisire più informazioni su etichette meno comuni.
   Inoltre, è possibile utilizzare una varietà di funzioni di caratteristiche che catturano meglio le relazioni tra le
   parole e le etichette.

In sintesi, il label bias è una tendenza intrinseca nei modelli di etichettatura sequenziale come il MEMM a preferire
etichette comuni e a lottare con etichette rare. Comprendere questo bias è importante quando si addestra e si utilizza
un MEMM, e spesso richiede l'adozione di strategie aggiuntive per migliorare le prestazioni del modello su etichette
meno comuni.

---

## Conditional Random Fields

I Conditional Random Fields (CRF), o Campi Aleatori Condizionati, sono un tipo di modello grafico probabilistico
utilizzato nell'ambito dell'elaborazione del linguaggio naturale (NLP) e in altre applicazioni di etichettatura
sequenziale, come il riconoscimento delle entità nominate, il riconoscimento delle parti del discorso e la segmentazione
delle sequenze.

Ecco una spiegazione dei CRF e del concetto di "global normalization":

**Conditional Random Fields (CRF):**

- I CRF sono utilizzati per il task di etichettatura sequenziale, in cui si assegnano etichette a ciascun elemento di
  una sequenza, come parole in una frase o caratteri in una parola.
- La caratteristica distintiva dei CRF è la loro capacità di modellare le dipendenze tra le etichette nelle sequenze. A
  differenza di modelli come i Maximum Entropy Markov Models (MEMM), i CRF utilizzano una funzione di partizione globale
  per garantire una distribuzione di probabilità coerente su tutte le possibili sequenze di etichette.
- I CRF sono modelli discriminativi, il che significa che si concentrano su modellare direttamente la probabilità
  condizionata delle etichette date le osservazioni, piuttosto che modellare congiuntamente le etichette e le
  osservazioni, come fanno i modelli generativi.

**Global Normalization (Normalizzazione Globale):**

- La "global normalization" (normalizzazione globale) è un aspetto chiave dei CRF. In pratica, significa che il CRF
  tiene conto di tutte le possibili sequenze di etichette quando calcola la probabilità condizionata delle etichette
  date le osservazioni.
- Questo approccio assicura che la somma delle probabilità per tutte le possibili sequenze di etichette sia uguale a 1,
  garantendo una distribuzione di probabilità coerente.
- La normalizzazione globale è importante perché tiene conto delle dipendenze globali tra le etichette, consentendo al
  modello di fare previsioni coerenti sull'intera sequenza, evitando bias locali che potrebbero verificarsi in modelli
  che considerano solo dipendenze locali tra etichette adiacenti.

In sintesi, i Conditional Random Fields (CRF) sono modelli probabilistici utilizzati per l'etichettatura sequenziale,
che tengono conto delle dipendenze globali tra le etichette nelle sequenze. La "global normalization" è un aspetto
fondamentale dei CRF che assicura una distribuzione di probabilità coerente su tutte le possibili sequenze di etichette,
migliorando la coerenza e l'accuratezza delle previsioni del modello.

---

## BiRNN: Bidirectional Recurrent Neural Networks

Le BiRNN, o Bidirectional Recurrent Neural Networks, sono una variante delle RNN (Recurrent Neural Networks) che
differiscono dalle RNN tradizionali nella loro capacità di catturare informazioni dal contesto sia passato che futuro in
una sequenza di dati. Ecco come funzionano e in che modo si differenziano dalle RNN:

**RNN (Recurrent Neural Network):**

- Le RNN sono un tipo di rete neurale utilizzate per modellare dati sequenziali, come sequenze di parole in un testo o
  serie temporali.
- In una RNN tradizionale, ogni nodo (o unità) nella rete ha uno stato nascosto che riceve input dallo stato nascosto
  precedente nella sequenza. Questo permette alla RNN di catturare le dipendenze temporali nei dati sequenziali.
- Tuttavia, le RNN tradizionali sono limitate nel senso che considerano solo il contesto passato mentre elaborano una
  sequenza. Questo significa che possono mancare informazioni dal contesto futuro che potrebbero essere utili in alcune
  applicazioni.

**BiRNN (Bidirectional Recurrent Neural Network):**

- Le BiRNN sono state progettate per superare la limitazione delle RNN tradizionali nella considerazione solo del
  contesto passato. Le BiRNN incorporano due reti RNN separate, una che elabora la sequenza dall'inizio alla fine (
  chiamata RNN in avanti) e l'altra che elabora la sequenza dall'ultimo elemento al primo (chiamata RNN all'indietro).
- Le uscite delle due reti vengono combinate in ciascun passo temporale, consentendo alla BiRNN di catturare
  informazioni sia dal passato che dal futuro durante la sua elaborazione.
- Questa capacità di considerare il contesto sia passato che futuro rende le BiRNN particolarmente utili in applicazioni
  in cui le informazioni da entrambi i lati di un punto nella sequenza sono importanti. Ad esempio, nelle applicazioni
  di NLP, possono essere utilizzate per il riconoscimento delle entità nominate, il riconoscimento delle parti del
  discorso e altre attività di etichettatura sequenziale.

In sintesi, le BiRNN sono una variante delle RNN che incorporano due reti RNN separate (una in avanti e una
all'indietro) per catturare informazioni da entrambi i lati di una sequenza. Questa caratteristica le rende adatte a
compiti in cui il contesto sia passato che futuro è importante per ottenere previsioni accurate.

---

## LSTM: Long Short Term Memory

Le LSTM, o Long Short-Term Memory, sono un tipo di architettura di rete neurale ricorrente (RNN) progettata per gestire
problemi in cui la dipendenza a lungo termine tra gli elementi di una sequenza è cruciale. Sono particolarmente efficaci
nel trattare il problema del "vantaggio a breve termine" e del "gradiente che svanisce" nelle RNN tradizionali.

Ecco alcune caratteristiche chiave delle LSTM:

1. **Memoria a Lungo Termine**: Le LSTM sono dotate di una "memoria a lungo termine" interna che può memorizzare
   informazioni da eventi passati nella sequenza per lungo tempo. Questa memoria permette alle LSTM di catturare
   dipendenze a lungo termine, come ad esempio la struttura grammaticale in un testo o i modelli in una serie temporale.

2. **Porte di Controllo**: Le LSTM utilizzano porte di controllo, come le porte di input, di output e di dimenticanza,
   per regolare il flusso delle informazioni nella cella di memoria. Queste porte controllano quali informazioni devono
   essere aggiornate, dimenticate o passate alla memoria a lungo termine in base al contesto e alle informazioni
   correnti.

3. **Prevenzione del Gradients Vanishing**: Un problema comune nelle RNN tradizionali è il "gradiente che svanisce"
   durante l'addestramento, che rende difficile la cattura di dipendenze a lungo termine. Le LSTM mitigano questo
   problema grazie alle loro porte di controllo, che consentono al gradiente di propagarsi efficacemente attraverso la
   rete.

4. **Applicazioni**: Le LSTM sono ampiamente utilizzate in una serie di applicazioni, tra cui il riconoscimento di
   lingua parlata, il riconoscimento di scrittura a mano, la traduzione automatica, il riconoscimento di entità
   nominate, la generazione di testi coerenti e molte altre attività di elaborazione del linguaggio naturale e
   sequenziali.

5. **Struttura di Base**: Una LSTM è composta da uno o più strati di celle LSTM. Ogni cella LSTM contiene le porte di
   controllo e la memoria a lungo termine, ed è collegata alle celle nella sequenza temporale.

In sintesi, le LSTM sono una forma avanzata di rete neurale ricorrente progettata per gestire sequenze di dati
mantenendo una memoria a lungo termine, superando problemi come il gradiente che svanisce e consentendo di catturare
dipendenze a lungo termine tra gli elementi della sequenza. Queste reti sono fondamentali in molte applicazioni di
elaborazione del linguaggio naturale e di analisi di sequenze temporali.

### Gate LSTM

I gate nelle LSTM (Long Short-Term Memory) sono componenti critici che permettono alle LSTM di regolare il flusso delle
informazioni all'interno delle celle LSTM stesse. Questi gate svolgono un ruolo fondamentale nel controllo
dell'aggiornamento della memoria a lungo termine e nel mantenimento delle informazioni rilevanti per le previsioni.

Ecco una spiegazione più dettagliata dei gate LSTM e del loro ruolo:

1. **Gate di Input (Input Gate):**
    - Il gate di input controlla quali nuove informazioni dovrebbero essere aggiunte alla memoria a lungo termine della
      LSTM.
    - In base all'input corrente e allo stato nascosto precedente, il gate di input determina quali informazioni sono
      rilevanti e le memorizza nella memoria a lungo termine. Può essere visto come un filtro che seleziona le
      informazioni da conservare.

2. **Gate di Output (Output Gate):**
    - Il gate di output controlla quali informazioni dalla memoria a lungo termine dovrebbero essere incluse nell'output
      corrente della LSTM.
    - In base allo stato nascosto corrente e alle informazioni dalla memoria a lungo termine, il gate di output decide
      quali informazioni sono pertinenti per l'output e le estrae. Questo permette alla LSTM di produrre previsioni
      basate su informazioni rilevanti.

3. **Gate di Dimenticanza (Forget Gate):**
    - Il gate di dimenticanza decide quali informazioni nella memoria a lungo termine dovrebbero essere dimenticate o
      scartate.
    - Questo gate è responsabile di evitare che la memoria a lungo termine diventi saturata con informazioni obsolete.
      Può "imparare" a scartare informazioni non più rilevanti in modo che la memoria a lungo termine possa essere
      mantenuta più pulita.

In sostanza, i gate LSTM sono componenti dinamici che regolano l'aggiornamento e l'accesso alla memoria a lungo termine
della rete. Questa capacità di controllare il flusso delle informazioni è ciò che rende le LSTM efficaci nel catturare
dipendenze a lungo termine nelle sequenze di dati, rendendole adatte a una vasta gamma di applicazioni di elaborazione
del linguaggio naturale e analisi di sequenze temporali. I gate assolvono a un ruolo fondamentale nel consentire alle
LSTM di apprendere e conservare informazioni rilevanti per compiti complessi di predizione e classificazione.

---

## GRU: Gated Recurrent Unit

Una GRU, o Gated Recurrent Unit, è un tipo di architettura di rete neurale ricorrente (RNN) utilizzata per modellare
sequenze di dati, come testi o serie temporali. La GRU è strettamente correlata alle LSTM (Long Short-Term Memory),
un'altra forma di RNN avanzata.

Ecco alcune caratteristiche chiave delle GRU e la loro relazione con le LSTM:

**GRU (Gated Recurrent Unit):**

1. **Semplificazione delle LSTM**: La GRU è stata progettata come una semplificazione delle LSTM. Mentre le LSTM hanno
   tre porte (input, output e dimenticanza) per controllare il flusso delle informazioni all'interno della cella, le GRU
   hanno solo due porte principali: una porta di aggiornamento (update gate) e una porta di reset.

2. **Gate di Aggiornamento (Update Gate)**: La porta di aggiornamento nelle GRU determina quanto delle informazioni
   precedenti nella cella di stato dovrebbe essere mantenuto e quanto dovrebbe essere aggiornato con le nuove
   informazioni correnti. Questo rende le GRU più efficienti in termini computazionali rispetto alle LSTM, che hanno una
   porta di dimenticanza separata.

3. **Gate di Reset (Reset Gate)**: La porta di reset nelle GRU controlla quanto delle informazioni passate dovrebbero
   essere dimenticate o "resettate" quando si elabora l'input corrente. Questo aiuta a gestire il flusso delle
   informazioni a lungo termine.

4. **Memoria a Corto Termine (Short-Term Memory)**: Mentre le LSTM hanno una memoria a lungo termine esplicita, le GRU
   mantengono solo una memoria a corto termine. Questa semplificazione può rendere le GRU più facili da addestrare in
   alcuni casi.

**Relazione con le LSTM:**

- Le GRU e le LSTM condividono l'obiettivo principale di affrontare il problema del "gradiente che svanisce" nelle RNN
  tradizionali e di catturare dipendenze a lungo termine nelle sequenze.
- Entrambe utilizzano meccanismi di gate per regolare il flusso delle informazioni all'interno della rete.
- La principale differenza è la complessità delle architetture: le GRU sono più semplici delle LSTM e hanno meno
  parametri. Questo le rende spesso una scelta più leggera dal punto di vista computazionale e più semplice da
  addestrare, ma può anche renderle meno potenti in alcune situazioni in cui è necessario catturare relazioni molto
  complesse nelle sequenze.

In sintesi, le GRU sono una variante delle LSTM, progettate per semplificare l'architettura delle reti neurali
ricorrenti mantenendo al contempo la capacità di catturare dipendenze a lungo termine nelle sequenze. La scelta tra LSTM
e GRU dipende spesso dalle esigenze specifiche del problema e dalle risorse computazionali a disposizione.

---

## Padding

Il "padding" è una tecnica utilizzata nell'elaborazione del linguaggio naturale (NLP) e nell'apprendimento automatico
per gestire sequenze di dati di lunghezza variabile. Consiste nell'aggiunta di elementi speciali, noti come "padding
tokens" o semplicemente "padding," a una sequenza in modo che tutte le sequenze abbiano la stessa lunghezza. Questo è
particolarmente utile quando si lavora con modelli neurali che richiedono input di dimensioni uniformi, come le reti
neurali ricorrenti (RNN) o le reti neurali convoluzionali (CNN).

Ecco perché il padding è importante e come funziona:

1. **Uniformità delle Dimensioni**: Molte reti neurali, in particolare le RNN e le CNN, richiedono che gli input abbiano
   dimensioni uniformi. Ciò significa che tutte le sequenze di input devono avere la stessa lunghezza.

2. **Gestione di Sequenze di Lunghezza Diverse**: Nelle applicazioni di NLP, come il riconoscimento di entità nominate o
   il riconoscimento delle parti del discorso, le frasi possono avere lunghezze diverse. Il padding consente di gestire
   queste differenze in lunghezza.

3. **Token di Padding**: Per rendere tutte le sequenze della stessa lunghezza, vengono aggiunti token di padding alle
   sequenze più corte. Questi token di padding sono di solito caratteri speciali o token vuoti che non contengono
   informazioni utili per il modello. Ad esempio, si potrebbe usare un token come "<PAD>" o "<PAD>".

4. **Allineamento**: Le sequenze vengono quindi allineate in modo che abbiano tutte la stessa lunghezza. Questo consente
   di creare un batch di dati di addestramento omogeneo da alimentare al modello.

5. **Maschere di Padding**: Durante l'addestramento, è importante utilizzare delle maschere di padding per informare il
   modello dei token di padding in modo che non vengano considerati nel calcolo delle previsioni. In questo modo, il
   modello non assegnerà importanza ai token di padding durante l'elaborazione.

In sostanza, il padding è una tecnica comune per rendere uniformi le dimensioni delle sequenze nell'apprendimento
automatico, consentendo ai modelli di lavorare con dati di lunghezza variabile. È particolarmente rilevante nelle
applicazioni di NLP in cui le frasi possono variare notevolmente in lunghezza, ma è utilizzato anche in altri contesti
in cui è necessario uniformare le dimensioni dei dati di input.

---

## Self attention

Il meccanismo di "self-attention," conosciuto anche come "self-attention mechanism" o "auto-attention," è un componente
chiave nelle moderne architetture di reti neurali per il trattamento del linguaggio naturale (NLP), in particolare nei
modelli di trasformatori come BERT, GPT e molti altri. Il meccanismo di self-attention è progettato per catturare le
relazioni e le dipendenze tra le parole o le posizioni all'interno di una sequenza.

Ecco come funziona il meccanismo di self-attention:

1. **Input Sequenziale**: Il meccanismo di self-attention prende in input una sequenza di elementi, come parole in una
   frase o vettori di features. Ogni elemento nella sequenza è rappresentato come un vettore, e la sequenza è
   rappresentata come una matrice, dove ogni riga corrisponde a un elemento e le colonne rappresentano le dimensioni del
   vettore di caratteristiche.

2. **Calcolo delle Pesi di Self-Attention**: Il meccanismo di self-attention calcola dei pesi (o attenzione) per
   ciascuna coppia di elementi nella sequenza. Questi pesi rappresentano quanto uno specifico elemento dovrebbe "
   attenere" o dare peso agli altri elementi nella sequenza. I pesi vengono calcolati in base alle similitudini tra gli
   elementi.

3. **Calcolo della Nuova Rappresentazione**: Una volta calcolati i pesi di self-attention, vengono utilizzati per
   combinare le informazioni degli altri elementi nella sequenza. In altre parole, ogni elemento "guarda" gli altri
   elementi e aggiorna la sua rappresentazione in base alle informazioni rilevanti contenute negli altri elementi.
   Questo calcolo produce una nuova rappresentazione per ciascun elemento.

4. **Apprendimento dei Pesi**: I pesi di self-attention non sono fissi ma sono appresi durante il processo di
   addestramento della rete neurale. Il modello impara quali relazioni tra gli elementi sono importanti per la specifica
   attività, e di conseguenza, adatta i pesi di self-attention di conseguenza.

5. **Multi-Head Attention**: Nei modelli di trasformatori, spesso viene utilizzato un meccanismo di self-attention "
   multi-head," che calcola i pesi di attenzione da diverse proiezioni dei vettori di input. Questo consente al modello
   di catturare diverse relazioni tra gli elementi in modo più ricco.

Il meccanismo di self-attention è potente perché consente al modello di considerare tutte le posizioni nella sequenza
contemporaneamente e di catturare dipendenze a lungo termine e relazioni complesse tra gli elementi. Questa capacità di
catturare informazioni contestuali è fondamentale per molte attività di NLP, come il riconoscimento delle entità
nominate, la traduzione automatica e la generazione di testo coerente. Il meccanismo di self-attention ha reso i modelli
di trasformatori molto efficaci nell'elaborazione del linguaggio naturale.

### Attention (classica) vs Self-Attention

L'attention classica, anche nota come "attention semplice" o "attention globale," è un meccanismo di attenzione
utilizzato in vari contesti, incluso l'apprendimento automatico e l'elaborazione del linguaggio naturale. La differenza
principale tra l'attention classica e il meccanismo di self-attention (o "multi-head attention") risiede nel modo in cui
gestiscono l'attenzione e le dipendenze tra gli elementi in una sequenza.

Ecco le principali differenze tra l'attention classica e il self-attention:

1. **Scope dell'Attenzione**:

    - **Attention Classica**: Nell'attention classica, un elemento in una sequenza (ad esempio, una parola in una frase)
      calcola l'attenzione rispetto a tutti gli altri elementi nella sequenza utilizzando una singola distribuzione di
      probabilità. Questo significa che ogni elemento è influenzato da tutti gli altri elementi in modo uniforme.

    - **Self-Attention**: Nel self-attention, ciascun elemento calcola l'attenzione rispetto a tutti gli altri elementi
      nella sequenza, ma lo fa individualmente. Ciò significa che l'attenzione può variare da elemento a elemento e può
      essere personalizzata in base alla relazione tra ciascun elemento e gli altri elementi.

2. **Parole Chiave e Valori**:

    - **Attention Classica**: Nell'attention classica, spesso si utilizzano vettori di "query," "chiavi" e "valori" per
      calcolare l'attenzione. Le "chiavi" e i "valori" sono solitamente gli stessi per tutti gli elementi nella
      sequenza, mentre le "query" possono variare.

    - **Self-Attention**: Nel self-attention, ciascun elemento ha i suoi vettori di "query," "chiavi" e "valori." Ciò
      consente di personalizzare l'attenzione per ciascun elemento, in modo che possa concentrarsi su ciò che è
      rilevante per quel particolare elemento.

3. **Complessità Computazionale**:

    - **Attention Classica**: L'attention classica può essere computazionalmente più leggera rispetto al self-attention,
      poiché coinvolge meno parametri e calcoli. È spesso utilizzata in modelli più semplici.

    - **Self-Attention**: Il self-attention, specialmente in forme "multi-head," può essere più computazionalmente
      costoso poiché coinvolge una maggiore quantità di calcoli e parametri. Tuttavia, è più potente nella cattura di
      relazioni complesse.

In sintesi, l'attention classica calcola un'unica attenzione condivisa da tutti gli elementi nella sequenza, mentre il
self-attention calcola un'attenzione individuale per ciascun elemento. Il self-attention è più flessibile e potente, ma
può essere più costoso computazionalmente. La scelta tra i due dipende dalle esigenze specifiche dell'applicazione e
dalle risorse a disposizione.