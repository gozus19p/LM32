# Text Classification - DNN in NLP

### Indice degli argomenti

* [Text Classification - DNN in NLP](#text-classification---dnn-in-nlp)
    * [Indice degli argomenti](#indice-degli-argomenti)
    * [Deep Neural Network](#deep-neural-network)
        * [Esempio di DNN in pseudocodice](#esempio-di-dnn-in-pseudocodice)
        * [Differenze tra reti neurali e reti neurali profonde (NN vs DNN)](#differenze-tra-reti-neurali-e-reti-neurali-profonde-nn-vs-dnn)
    * [DNN: Neuroni](#dnn-neuroni)
    * [DNN: strati (layers)](#dnn-strati-layers)
    * [DNN: pesi e bias](#dnn-pesi-e-bias)
    * [DNN: funzioni di attivazione](#dnn-funzioni-di-attivazione)
    * [DNN: funzioni di perdita (loss function)](#dnn-funzioni-di-perdita-loss-function)
    * [DNN: algoritmo di ottimizzazione](#dnn-algoritmo-di-ottimizzazione)
    * [Backpropagation](#backpropagation)
    * [Regularization](#regularization)
    * [Convolutional Neural Network (CNN)](#convolutional-neural-network-cnn)
        * [Global Average Pooling e Max Pooling](#global-average-pooling-e-max-pooling)
        * [Mappa delle caratteristiche](#mappa-delle-caratteristiche)
            * [Che cosa si intende per "caratteristica"](#che-cosa-si-intende-per-caratteristica)

---

## Deep Neural Network

Una rete neurale è composta da diversi elementi fondamentali e organizzata in strati (layers). Ecco una panoramica di
come funziona e da cosa è composta una rete neurale:

1. **Neuroni**: I neuroni sono l'unità fondamentale di una rete neurale. Ogni neurone riceve input, li elabora e produce
   un output. Gli input sono pesati e sommati prima di essere passati attraverso una funzione di attivazione. Il
   risultato dell'attivazione viene quindi inviato agli strati successivi.

2. **Strati (Layers)**: Una rete neurale è organizzata in strati. Ci sono tre tipi di strati principali:

    - **Input Layer**: Questo strato riceve i dati in input alla rete. Ogni nodo in questo strato rappresenta una
      caratteristica o una variabile dell'input.

    - **Hidden Layers**: Questi strati intermedi tra l'input e l'output eseguono il calcolo e l'elaborazione principale.
      Ogni neurone in uno strato nascosto è connesso a tutti i neuroni nello strato precedente e successivo.

    - **Output Layer**: Questo strato produce l'output previsto dalla rete. Il numero di nodi in questo strato dipende
      dal tipo di problema che la rete neurale sta risolvendo. Ad esempio, in un problema di classificazione binaria,
      potrebbe avere un solo neurone che rappresenta la probabilità di appartenenza a una classe.

3. **Pesi e Bias**: Ogni connessione tra i neuroni ha un peso associato. I pesi controllano l'importanza delle
   connessioni. Inoltre, ogni neurone ha un termine di bias che può essere aggiunto all'output pesato prima di applicare
   la funzione di attivazione. L'addestramento di una rete neurale comporta l'ottimizzazione di questi pesi e bias per
   adattarsi ai dati.

4. **Funzioni di Attivazione**: Le funzioni di attivazione introducono non linearità nella rete neurale. Sono utilizzate
   per introdurre complessità e catturare relazioni non lineari nei dati. Alcune delle funzioni di attivazione comuni
   includono ReLU (Rectified Linear Unit), Sigmoide e Tanh.

5. **Funzione di Perdita (Loss Function)**: Questa funzione misura quanto le previsioni della rete neurale si discostano
   dai risultati desiderati. L'obiettivo dell'addestramento è minimizzare questa funzione di perdita.

6. **Algoritmo di Ottimizzazione**: Gli algoritmi di ottimizzazione, come la discesa del gradiente, vengono utilizzati
   per aggiornare i pesi e i bias della rete al fine di ridurre la perdita durante il processo di addestramento.

Il funzionamento di una rete neurale coinvolge il passaggio di dati dall'input attraverso i vari strati, con calcoli e
aggiornamenti dei pesi lungo il percorso. L'addestramento è un processo iterativo in cui la rete migliora gradualmente
le sue prestazioni regolando i pesi e i bias in risposta ai dati di addestramento. Una volta addestrata, la rete può
essere utilizzata per fare previsioni su nuovi dati.

### Esempio di DNN in pseudocodice

```text
# Definizione dell'architettura della DNN
InputLayer: 
  # I nodi di input rappresentano le caratteristiche dell'input
  neurone1 = input1
  neurone2 = input2
  neurone3 = input3
  # ...

HiddenLayer:
  # I neuroni in uno strato nascosto eseguono calcoli intermedi
  # Ogni neurone riceve input da tutti i neuroni nel layer precedente
  neurone1 = funzione_di_attivazione(peso1 * neurone1_precedente + peso2 * neurone2_precedente + peso3 * neurone3_precedente + bias1)
  neurone2 = funzione_di_attivazione(peso4 * neurone1_precedente + peso5 * neurone2_precedente + peso6 * neurone3_precedente + bias2)
  neurone3 = funzione_di_attivazione(peso7 * neurone1_precedente + peso8 * neurone2_precedente + peso9 * neurone3_precedente + bias3)
  # ...

OutputLayer:
  # Il layer di output produce il risultato previsto
  output = funzione_di_attivazione(peso10 * neurone1_strato_nascosto + peso11 * neurone2_strato_nascosto + peso12 * neurone3_strato_nascosto + bias_output)

# Addestramento della DNN
Addestramento:
  # Calcolare l'errore tra l'output previsto e l'output desiderato
  errore = funzione_di_perdita(output, output_desiderato)
  
  # Aggiornare i pesi e i bias utilizzando un algoritmo di ottimizzazione (es. discesa del gradiente)
  peso1 = peso1 - tasso_di_apprendimento * derivata_rispetto_a_peso1 * errore
  peso2 = peso2 - tasso_di_apprendimento * derivata_rispetto_a_peso2 * errore
  # ...

# Utilizzo della DNN addestrata per fare previsioni
Previsione:
  input = dati_di_test
  neurone1_precedente = input1
  neurone2_precedente = input2
  neurone3_precedente = input3
  # ...
  
  # Passaggio attraverso la rete neurale
  neurone1_strato_nascosto = funzione_di_attivazione(peso1 * neurone1_precedente + peso2 * neurone2_precedente + peso3 * neurone3_precedente + bias1)
  neurone2_strato_nascosto = funzione_di_attivazione(peso4 * neurone1_precedente + peso5 * neurone2_precedente + peso6 * neurone3_precedente + bias2)
  neurone3_strato_nascosto = funzione_di_attivazione(peso7 * neurone1_precedente + peso8 * neurone2_precedente + peso9 * neurone3_precedente + bias3)
  
  output = funzione_di_attivazione(peso10 * neurone1_strato_nascosto + peso11 * neurone2_strato_nascosto + peso12 * neurone3_strato_nascosto + bias_output)
  
  # L'output rappresenta la previsione della DNN
```

### Differenze tra reti neurali e reti neurali profonde (NN vs DNN)

Una rete neurale normale, anche chiamata **rete neurale feedforward**, è una rete in cui l'informazione si muove in una
direzione, dall'input attraverso uno o più strati intermedi (noti come strati nascosti) fino all'output. Ogni neurone in
un determinato strato è connesso a tutti i neuroni nello strato successivo.

D'altra parte, una "deep neural network" (rete neurale profonda) è un tipo di rete neurale che ha più di uno o due
strati intermedi tra l'input e l'output. In altre parole, una DNN è caratterizzata dalla presenza di molti strati
nascosti, ed è progettata per apprendere rappresentazioni complesse dai dati in modo graduale.

La principale differenza tra una rete neurale normale e una DNN è quindi la profondità. Le DNN sono in grado di
apprendere rappresentazioni più complesse e astratte dei dati grazie alla loro architettura profonda, il che le rende
particolarmente adatte per problemi complessi come il riconoscimento delle immagini, il riconoscimento del linguaggio
naturale e altre attività di elaborazione dei dati in cui è necessario catturare relazioni complesse tra le
caratteristiche.

In sintesi, la principale differenza è la profondità e la capacità di modellare dati complessi, mentre una rete neurale
normale è più superficiale e semplice.


---

## DNN: Neuroni

Il concetto di "neurone" in una rete neurale artificiale è una rappresentazione matematica semplificata di un neurone
biologico nel cervello umano. I neuroni nelle reti neurali artificiali sono unità di calcolo fondamentali e giocano un
ruolo cruciale nel processo di apprendimento automatico. Ecco una spiegazione dettagliata di come funziona un neurone in
una rete neurale:

1. **Input**: Un neurone riceve input da uno o più neuroni nei layer precedenti o direttamente dall'input dei dati.
   Questi input sono numeri reali, ognuno dei quali ha un peso associato che rappresenta l'importanza dell'input.

2. **Pesi**: Ogni connessione tra un input e il neurone ha un peso. I pesi controllano l'importanza di ciascun input
   nell'elaborazione del neurone. In altre parole, i pesi determinano quanto ciascun input influenzerà l'output del
   neurone. Durante l'addestramento, questi pesi vengono regolati in modo che la rete neurale possa imparare dai dati.

3. **Somma ponderata**: Il neurone esegue una somma ponderata degli input moltiplicati per i loro pesi associati. Questo
   calcolo è rappresentato dalla seguente formula matematica:

   Somma Ponderata = (input1 * peso1) + (input2 * peso2) + ... + (inputN * pesoN)

   Dove N rappresenta il numero di input.

4. **Bias**: Inoltre, al risultato della somma ponderata viene aggiunto un termine noto come bias. Il bias è un
   parametro aggiuntivo che permette al neurone di apprendere spostamenti verticali nei dati. Il bias consente alla rete
   neurale di adattarsi meglio ai dati. La somma ponderata più il bias viene quindi passata attraverso una funzione di
   attivazione.

5. **Funzione di Attivazione**: La funzione di attivazione è una funzione matematica che determina se e quanto il
   neurone deve attivarsi in risposta alla somma ponderata più il bias. Le funzioni di attivazione introducono non
   linearità nella rete e sono fondamentali per consentire alle reti neurali di apprendere da dati complessi. Alcune
   delle funzioni di attivazione comuni includono la funzione ReLU (Rectified Linear Unit), la funzione sigmoide e la
   funzione tanh.

6. **Output**: L'output del neurone, dopo aver passato la somma ponderata più il bias attraverso la funzione di
   attivazione, è il risultato finale del neurone. Questo output può essere passato a uno o più neuroni nei layer
   successivi, contribuendo così al processo di elaborazione dell'informazione nella rete neurale.

In sintesi, un neurone in una rete neurale è una piccola unità di calcolo che prende input pesati, esegue calcoli sulla
somma ponderata di questi input, aggiunge un bias e applica una funzione di attivazione per produrre un output. Insieme,
i neuroni in una rete neurale lavorano in parallelo per apprendere da dati complessi e risolvere una varietà di compiti
di apprendimento automatico.

---

## DNN: strati (layers)

Il concetto di "layer" (strato) è fondamentale nelle reti neurali artificiali ed è responsabile dell'organizzazione e
della struttura complessiva di una rete neurale. Ecco una spiegazione dettagliata di cosa sia un layer in una rete
neurale:

1. **Ruolo di un Layer**: Un layer è un gruppo di neuroni all'interno di una rete neurale che esegue una specifica
   operazione di elaborazione dei dati. Ogni layer ha una funzione ben definita nel processo di trasformazione
   dell'input in un output.

2. **Input Layer**: Il primo layer di una rete neurale è chiamato "Input Layer" (strato di input). Questo strato riceve
   i dati di input originali, che possono essere ad esempio immagini, testo o altri tipi di dati. Ogni nodo (o neurone)
   in questo strato rappresenta una caratteristica o una variabile dell'input.

3. **Hidden Layer**: Gli strati intermedi tra l'input layer e l'output layer sono chiamati "Hidden Layers" (strati
   nascosti). Questi strati eseguono calcoli intermedi e svolgono un ruolo cruciale nel catturare relazioni complesse
   nei dati. Il numero di hidden layer e il numero di neuroni in ciascun strato nascosto possono variare a seconda
   dell'architettura della rete neurale.

4. **Output Layer**: L'ultimo layer di una rete neurale è chiamato "Output Layer" (strato di output). Questo strato
   produce l'output previsto o la risposta alla quale la rete neurale sta cercando di arrivare. Il numero di neuroni in
   questo strato dipende dal tipo di problema che la rete sta risolvendo. Ad esempio, in una rete neurale per la
   classificazione, ci potrebbe essere un neurone per ogni classe di output.

5. **Connessioni tra i Layer**: Ogni neurone in uno strato è connesso a tutti i neuroni nello strato precedente e
   successivo attraverso connessioni pesate. Queste connessioni trasmettono l'output di un neurone a tutti i neuroni
   nello strato successivo. I pesi delle connessioni determinano l'importanza di ciascun input nell'elaborazione.

6. **Funzione di Attivazione**: Ogni neurone in un layer (ad eccezione dell'input layer) applica una funzione di
   attivazione alla somma ponderata dei suoi input. Questa funzione introduce non linearità nella rete e permette alla
   rete di imparare relazioni complesse nei dati.

7. **Addestramento e Apprendimento**: Durante l'addestramento, i pesi delle connessioni e, talvolta, i bias all'interno
   di ciascun layer vengono regolati per minimizzare la funzione di perdita, che rappresenta la differenza tra l'output
   previsto e l'output desiderato. Questo processo di apprendimento permette alla rete di adattarsi ai dati e di
   migliorare le sue prestazioni.

In sintesi, un layer in una rete neurale è una collezione di neuroni che eseguono una specifica operazione di
trasformazione dei dati. Gli input fluiscono attraverso i vari strati, vengono elaborati dai neuroni e producono un
output che rappresenta la risposta della rete al problema che sta cercando di risolvere. La combinazione di diversi
strati nascosti e l'addestramento delle connessioni tra di essi rende possibile l'apprendimento e l'elaborazione di dati
complessi.

---

## DNN: pesi e bias

I concetti di "pesi" e "bias" sono fondamentali nelle reti neurali e svolgono un ruolo cruciale nel processo di
apprendimento automatico. Ecco una spiegazione dettagliata di cosa siano i pesi e i bias in una rete neurale:

1. **Pesi (Weights)**:
    - I pesi sono parametri associati a ciascuna connessione tra i neuroni in due layer consecutivi di una rete neurale.
    - Ogni connessione tra un neurone di un layer e un neurone di un layer successivo ha un peso associato.
    - I pesi rappresentano l'importanza di ciascuna connessione. Valori più alti dei pesi indicano un'importanza
      maggiore, mentre valori più bassi indicano un'importanza minore.
    - Durante il processo di addestramento, i pesi vengono regolati per minimizzare l'errore tra l'output previsto dalla
      rete e l'output desiderato.
    - L'addestramento delle reti neurali comporta l'ottimizzazione dei pesi in modo che la rete possa imparare dai dati
      e fare previsioni accurate.

2. **Bias**:
    - Il bias è un parametro aggiuntivo associato a ciascun neurone in un layer, ad eccezione dell'input layer.
    - Il bias è un valore costante che viene aggiunto alla somma ponderata degli input di un neurone prima di applicare
      la funzione di attivazione.
    - Il bias permette ai neuroni di apprendere spostamenti verticali nei dati. In altre parole, consente alla rete di
      adattarsi meglio ai dati senza dover cambiare i pesi delle connessioni.
    - Durante l'addestramento, il bias viene anch'esso regolato per contribuire a minimizzare l'errore.

3. **Ruolo dei Pesi e del Bias**:
    - I pesi controllano quanto ciascun input influenzerà l'output di un neurone. Pesando diversamente gli input, un
      neurone può concentrarsi su informazioni più rilevanti per il compito.
    - Il bias permette di spostare la curva di attivazione di un neurone in su o in giù lungo l'asse verticale. Questo è
      particolarmente utile quando i dati non sono centrati in zero.

4. **Funzione di Attivazione e Pesatura**:
    - Durante il calcolo della somma ponderata degli input, ciascun input è moltiplicato per il suo peso associato.
    - La somma ponderata più il bias viene quindi passata attraverso una funzione di attivazione, che determina l'output
      finale del neurone.
    - I pesi e il bias influenzano come la funzione di attivazione risponde all'input.

In sintesi, i pesi e il bias sono parametri fondamentali nelle reti neurali artificiali che determinano come ciascun
neurone elabora gli input e produce un output. Questi parametri vengono ottimizzati durante l'addestramento per far sì
che la rete neurale possa apprendere dai dati e svolgere compiti specifici in modo più accurato.

---

## DNN: funzioni di attivazione

La "funzione di attivazione" è un componente chiave in una rete neurale che introduce non linearità nelle operazioni
eseguite dai neuroni. Questa non linearità è essenziale per consentire alle reti neurali di apprendere e rappresentare
relazioni complesse nei dati. Ecco una spiegazione dettagliata del concetto di funzione di attivazione:

1. **Ruolo della Funzione di Attivazione**:
    - La funzione di attivazione è applicata al risultato della somma ponderata degli input di un neurone, sommata al
      bias.
    - Il suo ruolo principale è determinare se e quanto il neurone deve "attivarsi" in risposta agli input ricevuti. In
      altre parole, stabilisce l'intensità della risposta del neurone.

2. **Non Linearità**:
    - Le funzioni di attivazione introducono non linearità nella rete neurale. Questo significa che la relazione tra
      l'input e l'output non è semplicemente una scala o una trasformazione lineare.
    - Senza non linearità, una rete neurale con molti strati nascosti sarebbe equivalente a una singola rete lineare, il
      che limiterebbe notevolmente la sua capacità di apprendimento.

3. **Funzioni di Attivazione Comuni**:
    - Ci sono diverse funzioni di attivazione comuni utilizzate nelle reti neurali. Alcune di queste includono:
        - **ReLU (Rectified Linear Unit)**: È una funzione semplice ma molto utilizzata che restituisce l'input se è
          positivo o zero, altrimenti restituisce zero. È definita come ReLU(x) = max(0, x).
        - **Sigmoide**: È una funzione S-shaped che produce output compresi tra 0 e 1. È spesso utilizzata per problemi
          di classificazione binaria. È definita come Sigmoide(x) = 1 / (1 + e^(-x)).
        - **Tanh (Tangente iperbolica)**: È simile alla sigmoide ma produce output compresi tra -1 e 1. È definita come
          Tanh(x) = (e^(2x) - 1) / (e^(2x) + 1).

4. **Scelta della Funzione di Attivazione**:
    - La scelta della funzione di attivazione dipende dal problema che la rete neurale sta risolvendo.
    - ReLU è spesso una scelta predefinita per gli strati nascosti a causa della sua semplicità ed efficacia nella
      maggior parte dei casi.
    - Sigmoide e Tanh sono utilizzati negli strati di output per problemi di classificazione binaria o multiclasse, in
      quanto producono output tra 0 e 1 o tra -1 e 1, rispettivamente.

5. **Ruolo nell'Apprendimento**:
    - La funzione di attivazione determina come i neuroni rispondono agli input e come le informazioni sono propagate
      attraverso la rete.
    - Durante l'addestramento, le derivate della funzione di attivazione vengono utilizzate per calcolare come i pesi e
      il bias influenzano l'errore della rete. Questo è fondamentale nel processo di ottimizzazione dei pesi durante
      l'addestramento.

In sintesi, la funzione di attivazione è un elemento critico nelle reti neurali che introduce non linearità e determina
la risposta di un neurone agli input. La sua scelta e la sua configurazione influenzano notevolmente le prestazioni e la
capacità di una rete neurale di apprendere da dati complessi.

---

## DNN: funzioni di perdita (loss function)

La "funzione di perdita" (o loss function) è una componente fondamentale nelle reti neurali e viene utilizzata per
misurare quanto le previsioni della rete neurale si discostano dai risultati desiderati o reali. Il suo ruolo principale
è fornire un feedback sulla qualità delle previsioni della rete, e questo feedback viene utilizzato durante
l'addestramento per ottimizzare i pesi e i bias della rete. Ecco una spiegazione dettagliata del concetto di funzione di
perdita:

1. **Misurare l'Errore**:
    - La funzione di perdita misura l'errore tra le previsioni della rete neurale e i risultati desiderati o reali (
      etichette) sui dati di addestramento.
    - L'obiettivo è calcolare un valore che rifletta quanto le previsioni della rete siano vicine ai valori reali.

2. **Ruolo nell'Addestramento**:
    - Durante l'addestramento di una rete neurale, l'obiettivo è minimizzare la funzione di perdita.
    - Gli algoritmi di ottimizzazione, come la discesa del gradiente, vengono utilizzati per regolare i pesi e i bias
      della rete in modo da ridurre progressivamente il valore della funzione di perdita.

3. **Tipi di Funzioni di Perdita**:
    - Il tipo di funzione di perdita dipende dal tipo di problema che la rete neurale sta risolvendo. Alcuni esempi di
      funzioni di perdita comuni includono:
        - **Errore Quadratico Medio (MSE - Mean Squared Error)**: È utilizzato in problemi di regressione e misura la
          differenza quadratica media tra le previsioni e i valori reali. È definita come MSE = Σ(yi - ŷi)² / n, dove yi
          sono i valori reali, ŷi sono le previsioni e n è il numero di esempi.
        - **Entropia Incrociata (Cross-Entropy)**: È utilizzata in problemi di classificazione e misura quanto le
          previsioni si discostano dalle etichette reali. È particolarmente comune nell'addestramento di reti neurali
          per classificazione. L'entropia incrociata può avere varianti come la "binary cross-entropy" per la
          classificazione binaria e la "categorical cross-entropy" per la classificazione multiclasse.
        - **Errore Assoluto Medio (MAE - Mean Absolute Error)**: È utilizzato in problemi di regressione e misura la
          differenza media assoluta tra le previsioni e i valori reali. È definita come MAE = Σ|yi - ŷi| / n.

4. **Scelta della Funzione di Perdita**:
    - La scelta della funzione di perdita dipende dal problema specifico che si sta affrontando.
    - È importante selezionare una funzione di perdita appropriata in modo che la rete possa apprendere in modo efficace
      e produrre previsioni di alta qualità.

5. **Valutazione delle Prestazioni**:
    - Durante l'addestramento, la funzione di perdita viene utilizzata per valutare le prestazioni della rete su dati di
      addestramento.
    - Inoltre, la funzione di perdita può essere utilizzata per valutare le prestazioni su dati di convalida o test per
      verificare la generalizzazione della rete.

In sintesi, la funzione di perdita è una metrica cruciale che misura l'errore tra le previsioni di una rete neurale e i
risultati desiderati. La sua minimizzazione è l'obiettivo principale dell'addestramento della rete e consente alla rete
di apprendere dai dati e di migliorare le sue prestazioni nel compito specifico.

---

## DNN: algoritmo di ottimizzazione

Un "algoritmo di ottimizzazione" è un insieme di procedure matematiche e regole che vengono utilizzate per regolare i
parametri di una rete neurale al fine di minimizzare una funzione di perdita specifica. L'obiettivo principale di questi
algoritmi è far convergere la rete neurale verso una configurazione in cui la funzione di perdita raggiunge il suo
valore minimo o si avvicina il più possibile a esso. Ecco una spiegazione dettagliata del concetto di algoritmo di
ottimizzazione:

1. **Ruolo nell'Addestramento**:
    - Durante l'addestramento di una rete neurale, l'obiettivo è trovare la combinazione migliore di pesi e bias che
      permetta alla rete di fare previsioni accurate sui dati di input.
    - Gli algoritmi di ottimizzazione sono utilizzati per regolare progressivamente questi parametri in modo che la
      funzione di perdita, che misura l'errore tra le previsioni e i valori desiderati, sia minimizzata.

2. **Discesa del Gradiente**:
    - La maggior parte degli algoritmi di ottimizzazione si basa sul concetto di "discesa del gradiente". Questo
      concetto utilizza il gradiente della funzione di perdita rispetto ai parametri (pesi e bias) per determinare la
      direzione e l'ampiezza delle correzioni da apportare ai parametri.
    - Il gradiente rappresenta la direzione in cui la funzione di perdita sta crescendo più rapidamente, quindi il suo
      opposto indica la direzione in cui la funzione sta diminuendo più rapidamente. L'obiettivo è muoversi nella
      direzione opposta al gradiente per ridurre la funzione di perdita.

3. **Learning Rate (Tasso di Apprendimento)**:
    - Un parametro fondamentale in molti algoritmi di ottimizzazione è il "learning rate" (tasso di apprendimento).
      Questo parametro controlla la dimensione dei passi che l'algoritmo compie lungo il gradiente.
    - Un learning rate troppo grande può far oscillare l'ottimizzazione o farla divergere, mentre un learning rate
      troppo piccolo può far convergere l'ottimizzazione molto lentamente.
    - La scelta del tasso di apprendimento è un'importante considerazione nell'utilizzo di algoritmi di ottimizzazione.

4. **Algoritmi di Ottimizzazione Comuni**:
    - Ci sono vari algoritmi di ottimizzazione ampiamente utilizzati. Alcuni di essi includono la "Stochastic Gradient
      Descent (SGD)", l'"Adam", il "RMSprop", il "Momentum", e altri.
    - Ogni algoritmo ha le sue caratteristiche e vantaggi, ed è spesso necessario sperimentare per determinare quale
      funziona meglio per un determinato problema.

5. **Monitoraggio della Convergenza**:
    - Durante l'addestramento, è importante monitorare la convergenza dell'algoritmo di ottimizzazione. Questo implica
      controllare come la funzione di perdita cambia nel tempo e come i pesi e i bias si aggiornano.
    - L'addestramento può essere interrotto quando la convergenza è raggiunta o quando si verifica un criterio di
      arresto predefinito.

In sintesi, un algoritmo di ottimizzazione è un insieme di procedure utilizzate per regolare i parametri di una rete
neurale al fine di minimizzare una funzione di perdita. Questi algoritmi sono fondamentali per l'addestramento delle
reti neurali e svolgono un ruolo cruciale nel consentire alla rete di apprendere dai dati e di adattarsi ai compiti
specifici. La scelta dell'algoritmo di ottimizzazione e dei suoi iperparametri è una parte importante del processo di
progettazione delle reti neurali.

---

## Backpropagation

"Backpropagation," abbreviazione di "backward propagation of errors" (propagazione all'indietro degli errori), è un
algoritmo fondamentale utilizzato nell'addestramento di reti neurali artificiali. Il suo scopo principale è calcolare i
gradienti della funzione di perdita rispetto ai pesi delle connessioni all'interno della rete neurale. Questi gradienti
vengono poi utilizzati per regolare i pesi in modo da minimizzare la funzione di perdita durante l'addestramento. Ecco
come funziona il processo di backpropagation:

1. **Inizializzazione**:
    - L'algoritmo inizia con una rete neurale con pesi iniziali casuali o inizializzati in modo specifico.
    - Viene selezionato un insieme di dati di addestramento che consiste in esempi di input e le rispettive etichette o
      risultati desiderati.

2. **Passaggio in Avanti (Forward Pass)**:
    - L'input viene fornito alla rete neurale.
    - Gli input passano attraverso i vari strati della rete, con ogni strato che esegue calcoli basati sui pesi delle
      connessioni e le funzioni di attivazione dei neuroni.
    - La rete produce un output o una previsione per ciascun esempio di input.

3. **Calcolo dell'Errore (Loss Calculation)**:
    - L'output della rete neurale viene confrontato con le etichette o i risultati desiderati per calcolare l'errore.
      Questo è spesso fatto utilizzando una funzione di perdita, come l'errore quadratico medio (MSE) per problemi di
      regressione o l'entropia incrociata per problemi di classificazione.

4. **Backpropagation Proper (Retropropagazione)**:
    - Questa è la fase cruciale in cui si calcolano i gradienti dell'errore rispetto ai pesi delle connessioni all'
      interno della rete.
    - L'algoritmo inizia dallo strato di output e procede all'indietro attraverso gli strati nascosti della rete.
    - Per ciascun neurone, viene calcolato il gradiente dell'errore rispetto all'output del neurone e quindi il
      gradiente rispetto ai pesi e al bias di quel neurone.
    - I gradienti vengono calcolati utilizzando la regola della catena (regola del gradiente) che permette di propagare
      l'errore all'indietro attraverso la rete.

5. **Aggiornamento dei Pesos (Weight Update)**:
    - Una volta calcolati i gradienti, vengono utilizzati per aggiornare i pesi e i bias all'interno della rete neurale.
    - La direzione e l'ampiezza degli aggiornamenti dipendono dai gradienti e da un parametro chiamato tasso di
      apprendimento (learning rate).
    - Questo processo di aggiornamento viene eseguito iterativamente per l'intero set di dati di addestramento (o un
      mini-batch) e attraverso molte epoche fino a quando la funzione di perdita raggiunge un valore accettabile o si
      verifica un criterio di arresto.

6. **Ripetizione dell'Addestramento**:
    - Il processo di passaggio in avanti, calcolo dell'errore, backpropagation e aggiornamento dei pesi viene ripetuto
      fino a quando la rete raggiunge buone prestazioni sul set di dati di addestramento e/o convalida.

In sintesi, il backpropagation è un algoritmo di ottimizzazione utilizzato per addestrare reti neurali artificiali.
Attraverso il calcolo dei gradienti dell'errore rispetto ai pesi delle connessioni e l'aggiornamento iterativo dei pesi,
la rete apprende a migliorare le sue previsioni e a ridurre la funzione di perdita, avvicinandosi sempre di più ai
risultati desiderati durante l'addestramento.

---

## Regularization

La "regularizzazione" è una tecnica utilizzata nell'addestramento di modelli di machine learning, comprese le reti
neurali, con l'obiettivo di prevenire l'overfitting e migliorare la capacità di generalizzazione del modello.
L'overfitting si verifica quando un modello ha appreso troppo dai dati di addestramento e si adatta troppo strettamente
a essi, perdendo la capacità di fare previsioni accurate su dati non visti, come i dati di convalida o di test. La
regularizzazione introduce penalizzazioni o restrizioni addizionali sul modello durante l'addestramento per evitare l'
overfitting. Ecco alcune delle tecniche di regularizzazione più comuni:

1. **L1 e L2 Regularization**:
    - L1 e L2 sono due tipi di regolarizzazione basati sulla penalizzazione dei pesi del modello.
    - L1 regularization aggiunge una penalizzazione basata sulla somma dei valori assoluti dei pesi. Questo porta alcuni
      pesi a diventare esattamente zero, rendendo il modello sparso.
    - L2 regularization aggiunge una penalizzazione basata sulla somma dei quadrati dei pesi. Questo tende a ridurre
      tutti i pesi, ma in modo uniforme, senza annullarli completamente.

2. **Dropout**:
    - Il dropout è una tecnica di regularizzazione specifica per le reti neurali.
    - Durante l'addestramento, il dropout "disattiva" casualmente un numero specifico di neuroni in uno strato con una
      probabilità predeterminata. Ciò costringe la rete a imparare in modo più robusto, evitando l'overfitting.
    - Durante l'inferenza (quando il modello viene utilizzato per fare previsioni), il dropout non viene applicato, ma
      il modello tiene conto dell'effetto del dropout nell'output.

3. **Early Stopping**:
    - L'early stopping è una tecnica di regolarizzazione che prevede l'arresto dell'addestramento del modello prima che
      si verifichi l'overfitting.
    - Durante l'addestramento, si tiene traccia delle prestazioni del modello su un set di dati di convalida. Quando le
      prestazioni sulla convalida iniziano a peggiorare, l'addestramento viene interrotto.
    - Questo approccio impedisce al modello di "imparare troppo" dai dati di addestramento eccessivamente.

4. **Data Augmentation**:
    - La data augmentation è una forma indiretta di regularizzazione che consiste nel generare nuovi dati di
      addestramento da quelli esistenti attraverso trasformazioni come la rotazione, la traslazione e lo specchiamento.
    - Questo aumenta la varietà dei dati di addestramento, riducendo il rischio di overfitting.

5. **Riduzione delle Dimensioni**:
    - In alcuni casi, la regolarizzazione può essere ottenuta riducendo la complessità del modello, ad esempio riducendo
      il numero di unità in uno strato nascosto o il numero di feature in un modello.
    - Questo limita la capacità del modello di adattarsi eccessivamente ai dati di addestramento.

La scelta della tecnica di regularizzazione dipende dal problema specifico e dalla struttura del modello. L'obiettivo
principale della regularizzazione è garantire che il modello apprenda da dati in modo più generale e possa fare
previsioni accurate su dati non visti, migliorando così la sua capacità di generalizzazione.

---

## Convolutional Neural Network (CNN)

Le reti neurali convoluzionali (CNN), o Convolutional Neural Networks, sono un tipo di architettura di rete neurale
ampiamente utilizzata per il riconoscimento di pattern in dati bidimensionali, in particolare immagini. Le CNN sono
progettate per catturare automaticamente e in modo gerarchico le caratteristiche significative all'interno delle
immagini, rendendole molto efficaci nei compiti di visione artificiale. Ecco come funzionano le reti convoluzionali:

1. **Convoluzione**:
    - La caratteristica distintiva delle CNN è l'operazione di "convoluzione". Una convoluzione coinvolge una piccola
      finestra (chiamata filtro o kernel) che scorre sull'immagine di input.
    - In ogni posizione, il filtro esegue un prodotto scalare tra i valori dei pixel dell'immagine e i pesi del filtro.
      Il risultato viene sommato per produrre un valore singolo nella mappa delle caratteristiche (feature map).
    - L'operazione di convoluzione consente alla CNN di rilevare diverse caratteristiche locali, come bordi, linee e
      texture, a diverse posizioni nell'immagine.

2. **Strato di Pooling**:
    - Dopo l'operazione di convoluzione, viene spesso aggiunto uno strato di "pooling" (o sottocampionamento) per
      ridurre la dimensione delle mappe delle caratteristiche.
    - L'operazione di pooling riduce la dimensione spaziale delle mappe delle caratteristiche mantenendo le
      caratteristiche più importanti.
    - Il max-pooling è un tipo comune di pooling che seleziona il valore massimo all'interno di una finestra.

3. **Strati di Convoluzione e Pooling Multipli**:
    - Le CNN utilizzano solitamente diversi strati di convoluzione e pooling in sequenza. Questi strati creano una
      gerarchia di caratteristiche, permettendo alla rete di rilevare pattern sempre più complessi e astratti nelle
      immagini.

4. **Strati Fully Connected**:
    - Dopo i strati di convoluzione e pooling, le CNN spesso includono uno o più strati completamente connessi (fully
      connected) che combinano le caratteristiche estratte in una rappresentazione finale.
    - Questi strati possono essere seguiti da strati di uscita che producono le previsioni finali per il compito
      specifico, ad esempio il riconoscimento di oggetti in un'immagine.

5. **Funzione di Attivazione**:
    - In ciascun strato delle CNN, le attivazioni passano attraverso una funzione di attivazione, spesso la ReLU (
      Rectified Linear Unit), per introdurre non linearità nell'output.
    - Questa non linearità è importante per consentire alla rete di apprendere pattern complessi e non lineari.

6. **Addestramento**:
    - Le CNN vengono addestrate utilizzando un set di dati di addestramento in cui le immagini sono etichettate con le
      loro categorie o classi di appartenenza.
    - Durante l'addestramento, la rete ottimizza i pesi dei filtri e dei neuroni per minimizzare una funzione di perdita
      che misura la discrepanza tra le previsioni della rete e le etichette reali.
    - Questa ottimizzazione viene eseguita utilizzando algoritmi di ottimizzazione come la discesa del gradiente.

7. **Convoluzione 2D vs. Convoluzione 1D**:
    - Mentre le spiegazioni sopra si riferiscono alla convoluzione bidimensionale utilizzata nelle immagini, le CNN
      possono anche essere utilizzate per dati unidimensionali, ad esempio sequenze di testo o segnali audio,
      utilizzando la convoluzione unidimensionale.

In sintesi, le reti neurali convoluzionali sono progettate specificamente per il riconoscimento di pattern in dati
bidimensionali, come le immagini. Utilizzano l'operazione di convoluzione per estrarre caratteristiche locali e
sottocampionamento per ridurre la dimensione delle mappe delle caratteristiche. Questa architettura si è dimostrata
estremamente efficace in una vasta gamma di applicazioni di visione artificiale, tra cui il riconoscimento di oggetti,
il riconoscimento facciale, la segmentazione delle immagini e molto altro.

### Global Average Pooling e Max Pooling

Il "max pooling" e il "global average pooling" sono due tecniche utilizzate negli strati di pooling delle reti neurali
convoluzionali (CNN) per ridurre la dimensione delle mappe delle caratteristiche generate dagli strati di convoluzione.
Entrambe svolgono un ruolo importante nella semplificazione delle rappresentazioni delle caratteristiche estratte dalle
immagini, ma ci sono differenze chiave tra le due.

**Max Pooling**:

- Nel max pooling, una finestra (solitamente 2x2 o 3x3) scorre attraverso la mappa delle caratteristiche.
- In ogni posizione, il valore massimo all'interno della finestra viene selezionato e utilizzato come valore di uscita
  per quella regione.
- Il max pooling estrae le caratteristiche più rilevanti in ogni regione, mantenendo solo il valore massimo e scartando
  gli altri.
- È utile per conservare le caratteristiche dominanti e ignorare i dettagli meno rilevanti.
- Questa operazione di pooling è eseguita separatamente su ciascuna mappa delle caratteristiche, riducendo le dimensioni
  spaziali di ciascuna.

**Global Average Pooling**:

- Il global average pooling (GAP), al contrario, non utilizza finestre di pooling.
- Invece, esegue una media di tutti i valori in ciascuna mappa delle caratteristiche.
- Il risultato è un singolo valore di uscita per ciascuna mappa delle caratteristiche, che rappresenta la media di tutte
  le sue caratteristiche.
- Il GAP riduce drasticamente le dimensioni spaziali della mappa delle caratteristiche a un singolo valore per mappa.
- Questa tecnica è spesso utilizzata nella parte finale di una CNN, prima dei livelli completamente connessi, per
  ottenere una rappresentazione compatta delle caratteristiche estratte.

**Differenze chiave tra Max Pooling e Global Average Pooling**:

1. **Operazione**:
    - Il max pooling seleziona il valore massimo in ciascuna finestra, mentre il global average pooling calcola la media
      di tutti i valori in una mappa delle caratteristiche.

2. **Dimensioni di Uscita**:
    - Il max pooling produce una mappa delle caratteristiche con le stesse dimensioni spaziali ma meno canali (feature
      maps), mentre il global average pooling riduce la dimensione spaziale a un singolo valore per ciascuna mappa delle
      caratteristiche.

3. **Riduzione delle Dimensioni**:
    - Il max pooling è efficace per ridurre la dimensione spaziale mantenendo le caratteristiche più significative.
    - Il global average pooling è utile per ottenere una rappresentazione compatta di ogni mappa delle caratteristiche,
      riducendo notevolmente le dimensioni.

4. **Utilizzo**:
    - Il max pooling è spesso utilizzato in vari strati intermedi di una CNN.
    - Il global average pooling è comunemente utilizzato come layer finale prima dei livelli completamente connessi in
      alcune architetture di CNN.

In generale, la scelta tra max pooling e global average pooling dipende dall'architettura della rete e dalla natura del
problema. Entrambi sono efficaci nel semplificare le rappresentazioni delle caratteristiche, ma il max pooling è più
adatto per conservare caratteristiche dominanti, mentre il global average pooling è utile per ottenere rappresentazioni
più compatte.

### Mappa delle caratteristiche

Una "mappa delle caratteristiche" (feature map), anche nota come "mappa di attivazione," è una rappresentazione
bidimensionale delle caratteristiche estratte da una rete neurale convoluzionale (CNN) durante l'elaborazione di un'
immagine o di un'altra forma di dato bidimensionale. Queste mappe sono il risultato dell'applicazione di filtri (kernel)
convoluti ai dati di input attraverso le operazioni di convoluzione e pooling. Ecco alcune caratteristiche chiave delle
mappe delle caratteristiche:

1. **Dimensioni Spaziali**: Una mappa delle caratteristiche ha una dimensione spaziale bidimensionale che rappresenta
   l'area dell'immagine o del dato bidimensionale elaborato. Ad esempio, se l'immagine di input è 128x128 pixel, le
   mappe delle caratteristiche avranno dimensioni spaziali relative a queste dimensioni.

2. **Canali (Feature Channels)**: Le mappe delle caratteristiche possono essere costituite da più canali, o feature
   channels. Ciascun canale rappresenta un diverso aspetto o una diversa caratteristica estratta dall'input. Ad esempio,
   un canale potrebbe rappresentare bordi, un altro texture, un altro colori, e così via.

3. **Estrazione delle Caratteristiche**: Ogni punto (pixel) in una mappa delle caratteristiche rappresenta l'output di
   uno specifico filtro (kernel) applicato a una porzione corrispondente dell'input. L'intensità di ciascun punto
   riflette quanto la caratteristica associata al filtro è presente in quella regione dell'input.

4. **Convoluzione**: Le mappe delle caratteristiche vengono generate attraverso l'operazione di convoluzione, in cui i
   filtri convoluti scorrono sull'input e producono valori di output (pixel) per ciascuna posizione dell'input. Questa
   operazione estrae le caratteristiche locali rilevanti.

5. **Pooling**: Dopo l'operazione di convoluzione, le mappe delle caratteristiche possono essere ulteriormente elaborate
   attraverso l'operazione di pooling (spesso max pooling o average pooling). Il pooling riduce la dimensione spaziale
   delle mappe, mantenendo le caratteristiche più significative.

6. **Strati di Profondità**: In una CNN, le mappe delle caratteristiche vengono generate in strati di profondità
   crescente. Gli strati iniziali estraggono caratteristiche di basso livello, come bordi e linee, mentre gli strati più
   profondi estraggono caratteristiche di livello superiore e più complesse, come forme e oggetti.

7. **Informazione Gerarchica**: Le mappe delle caratteristiche estratte dagli strati inferiori catturano informazioni
   dettagliate, mentre quelle estratte dagli strati superiori contengono informazioni più astratte e semantiche.

8. **Utilizzo**:
    - Le mappe delle caratteristiche estratte da una CNN vengono utilizzate per compiti come il riconoscimento di
      oggetti, la segmentazione delle immagini, il riconoscimento facciale e molti altri problemi di visione
      artificiale.
    - In molti casi, le mappe delle caratteristiche vengono passate attraverso uno o più strati completamente connessi
      per effettuare la classificazione o la previsione finale.

In sintesi, le mappe delle caratteristiche sono rappresentazioni intermedie di un'immagine o di altri dati
bidimensionali estratte da una rete neurale convoluzionale. Contengono informazioni sulle caratteristiche rilevanti
presenti nell'input ed è attraverso queste mappe che una CNN impara a riconoscere pattern e oggetti nelle immagini
durante il processo di addestramento.

#### Che cosa si intende per "caratteristica"

In contesto di elaborazione delle immagini e machine learning, il termine "caratteristica" si riferisce a una proprietà
o un aspetto specifico di un'immagine o di un dato che può essere misurato o estratto per rappresentare informazioni
rilevanti. Queste caratteristiche sono fondamentali per descrivere, distinguere o analizzare gli oggetti o i pattern
presenti nei dati. Ecco alcune spiegazioni più dettagliate sul concetto di caratteristica:

1. **Estrazione di Caratteristiche**:
    - L'estrazione di caratteristiche consiste nel selezionare e misurare specifiche proprietà o pattern nei dati al
      fine di rappresentarli in modo più significativo e utile.
    - Nella visione artificiale, le caratteristiche possono essere estratte da immagini attraverso l'applicazione di
      filtri, operazioni di convoluzione e pooling, al fine di catturare bordi, texture, colori, forme e altri aspetti
      visivi.

2. **Tipi di Caratteristiche**:
    - Le caratteristiche possono essere di vario tipo, tra cui:
        - **Caratteristiche di Basso Livello**: Queste sono caratteristiche elementari come bordi, linee, angoli e
          colori.
        - **Caratteristiche di Livello Medio**: Queste rappresentano pattern più complessi come texture, forme
          geometriche e oggetti.
        - **Caratteristiche di Alto Livello**: Queste corrispondono a concetti o oggetti specifici, come un volto umano
          o un veicolo.
    - Le caratteristiche possono essere basate su pixel, segmenti di immagini o altre unità di dati rilevanti per il
      compito.

3. **Utilizzo**:
    - Le caratteristiche estratte da un insieme di dati vengono spesso utilizzate come input per algoritmi di machine
      learning o reti neurali, consentendo ai modelli di apprendere pattern e fare previsioni o classificazioni.
    - Ad esempio, nel riconoscimento di oggetti, le caratteristiche estratte da un'immagine possono includere la
      presenza di linee orizzontali, cerchi, ecc., che vengono utilizzate per identificare l'oggetto in questione.

4. **Feature Engineering**:
    - L'ingegneria delle caratteristiche è il processo di selezione e creazione di caratteristiche informative per
      migliorare le prestazioni dei modelli di machine learning.
    - Gli esperti di dominio spesso svolgono un ruolo cruciale nell'identificare le caratteristiche più rilevanti per un
      compito specifico.

5. **Applicazioni**:
    - Le caratteristiche sono utilizzate in una vasta gamma di applicazioni, tra cui il riconoscimento di oggetti, il
      riconoscimento facciale, la segmentazione delle immagini, l'analisi del testo e molti altri problemi di machine
      learning.

In sintesi, le caratteristiche sono rappresentazioni misurabili ed estratte dai dati che permettono ai modelli di
machine learning di catturare informazioni rilevanti per un compito specifico. L'identificazione e l'estrazione di
caratteristiche appropriate sono fondamentali per la creazione di modelli di machine learning efficaci e per l'analisi
dei dati in vari campi.