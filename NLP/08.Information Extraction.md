# Information Extraction

### Indice degli argomenti

* [Information Extraction](#information-extraction)
    * [Indice degli argomenti](#indice-degli-argomenti)
    * [Task](#task)
        * [Span di testo](#span-di-testo)
    * [Named Entity Recognition](#named-entity-recognition)
        * [Classi di entità](#classi-di-entità)
        * [ACE NER categories](#ace-ner-categories)
    * [Notazione IOB](#notazione-iob)
    * [Approcci di sequence labeling](#approcci-di-sequence-labeling)
        * [Approcci "feature-based"](#approcci-feature-based)
        * [Gazetteer](#gazetteer)
        * [BiLSTM](#bilstm)
    * [Entity Linking](#entity-linking)
    * [Relation Extraction](#relation-extraction)
        * [Approcci di relation extraction](#approcci-di-relation-extraction)

---

## Task

[~ Vai all'indice](#indice-degli-argomenti)

L'Information Extraction (IE) è un campo dell'elaborazione del linguaggio naturale (NLP) che si concentra sulla
estrazione di informazioni strutturate da testi non strutturati, come documenti o testi web. Questo processo coinvolge
l'identificazione e l'estrazione di informazioni specifiche, come entità (nomi di persone, luoghi, date), relazioni tra
entità e altri fatti rilevanti all'interno del testo.

Ecco alcuni dei principali task che l'Information Extraction assolve:

1. **Named Entity Recognition (NER)**: Questa è una delle componenti fondamentali dell'IE. L'NER identifica e classifica
   le entità nel testo, come persone, organizzazioni, luoghi, date, valute, ecc.

2. **Relation Extraction**: Questo task mira a identificare le relazioni tra diverse entità nel testo. Ad esempio,
   potrebbe cercare di estrarre relazioni come "è il presidente di" tra una persona e un'organizzazione.

3. **Event Extraction**: L'event extraction si concentra sull'individuazione e l'estrazione di eventi o azioni descritte
   nel testo insieme alle entità coinvolte e alle loro relazioni.

4. **Facts Extraction**: Questo può coinvolgere l'estrazione di fatti specifici da documenti, come informazioni
   statistiche, dati finanziari o qualsiasi tipo di informazione rilevante per uno scopo specifico.

5. **Summarization**: Anche se non è strettamente IE, la summarization può essere vista come un'estensione dell'IE, in
   quanto mira a estrarre le informazioni più rilevanti da un testo in modo conciso.

6. **Question Answering**: Anche in questo caso, l'IE può essere coinvolta nella fase di estrazione delle informazioni
   necessarie per rispondere a domande specifiche poste su un testo.

L'IE utilizza spesso tecniche di NLP avanzate, come il riconoscimento di pattern, l'apprendimento automatico, e l'uso di
corpora annotati per addestrare i modelli. È un campo in continua evoluzione e svolge un ruolo cruciale nell'estrazione
di conoscenza da grandi quantità di testo non strutturato, con applicazioni in molteplici settori, come l'analisi dei
dati, l'informatica biomedica, l'elaborazione automatica dei contratti e molto altro.

### Span di testo

Uno "span di testo" è un concetto utilizzato nell'ambito dell'elaborazione del linguaggio naturale (NLP) per indicare
una sequenza continua di parole o token all'interno di un testo. In altre parole, è una porzione di testo che comprende
una serie consecutiva di parole o caratteri. Gli span di testo possono variare in lunghezza, da una singola parola a una
frase completa o anche a un intero documento, a seconda del contesto e dell'obiettivo dell'analisi.

Gli span di testo vengono spesso utilizzati in NLP per identificare e manipolare parti specifiche di un testo. Ad
esempio, quando si effettua il riconoscimento di entità nominate (NER), l'obiettivo è individuare gli span di testo che
corrispondono a nomi di persone, organizzazioni, luoghi, ecc. Questi span di testo vengono quindi classificati come
entità.

Nel contesto del riconoscimento di relazioni tra entità o dell'estrazione di informazioni, è comune lavorare con span di
testo che rappresentano i concetti chiave di interesse e le relazioni tra di essi. Gli span di testo vengono inoltre
utilizzati nelle annotazioni linguistiche per specificare porzioni del testo su cui si sta lavorando, ad esempio
evidenziando un'area di testo per l'analisi grammaticale o semantica.

In sintesi, uno "span di testo" è un modo di rappresentare una parte del testo in modo coerente e continuo all'interno
dell'ambito dell'elaborazione del linguaggio naturale.

---

## Named Entity Recognition

[~ Vai all'indice](#indice-degli-argomenti)

La Named Entity Recognition (NER), o Riconoscimento delle Entità Nominative, è una tecnica nell'elaborazione del
linguaggio naturale (NLP) che si concentra sull'identificazione e l'etichettatura di entità specifiche all'interno di un
testo non strutturato. Queste entità possono includere nomi di persone, organizzazioni, luoghi, date, numeri di
telefono, valute e altre categorie di entità rilevanti per un'applicazione specifica. Ecco come funziona la NER:

1. **Preparazione del testo**: La NER inizia con la preparazione del testo, che può includere la tokenizzazione (la
   divisione del testo in parole o token), la rimozione di caratteri speciali, la normalizzazione delle maiuscole e
   minuscole, ecc.

2. **Selezione delle feature**: Vengono estratte caratteristiche (features) rilevanti da ciascun token nel testo. Queste
   caratteristiche possono includere informazioni sulla parola stessa (come la forma delle lettere maiuscole e
   minuscole), il contesto circostante (come le parole che lo circondano), e altre informazioni linguistiche.

3. **Addestramento del modello**: Un modello di machine learning, come un classificatore statistico o una rete neurale,
   viene addestrato su un insieme di dati annotati. Questi dati contengono esempi di testo in cui le entità sono state
   etichettate. Il modello impara a riconoscere pattern e associazioni tra le caratteristiche dei token e le etichette
   delle entità.

4. **Riconoscimento delle entità**: Una volta addestrato, il modello può essere utilizzato per analizzare nuovi testi e
   identificare le entità. Quando il modello incontra un token, assegna un'etichetta corrispondente se riconosce che il
   token è parte di un'entità nominativa. Ad esempio, se il modello identifica "Microsoft" in un testo, lo etichetterà
   come un'entità "ORG" (organizzazione).

5. **Post-processing**: In questa fase, è possibile applicare regole aggiuntive o tecniche di fusione per migliorare la
   coerenza delle entità estratte. Ad esempio, potrebbe essere necessario gestire casi in cui l'NER ha estratto "New"
   come parte di una data, ma "New" è anche un termine comune per nomi di luoghi.

6. **Valutazione e ottimizzazione**: È importante valutare le prestazioni del sistema NER utilizzando metriche come la
   precisione, il richiamo e la F1-score. In base ai risultati, è possibile apportare miglioramenti al modello o alle
   regole di post-processing per ottenere risultati migliori.

La NER è una componente fondamentale in molte applicazioni di NLP, tra cui il riconoscimento di informazioni rilevanti
in documenti, la classificazione di testi, l'estrazione di relazioni e molto altro. È utilizzata in una vasta gamma di
settori, tra cui il recupero di informazioni, l'analisi dei social media, l'elaborazione del linguaggio naturale
biomedica e altro ancora.

### Classi di entità

Nel riconoscimento delle entità nominative (NER), le entità vengono generalmente assegnate a diverse classi o categorie.
Le classi di entità possono variare leggermente a seconda del contesto o dell'applicazione specifica, ma ci sono alcune
categorie di entità comuni che vengono spesso utilizzate:

1. **PERSON**: Questa classe rappresenta nomi di persone. Ad esempio, "John Smith" o "Alice Johnson" sarebbero
   etichettati come entità della classe "PERSON".

2. **ORGANIZATION (ORG)**: Questa classe riguarda nomi di organizzazioni o aziende. Esempi includono "Microsoft",
   "Google" o "Università di Bologna".

3. **LOCATION (LOC)**: Le entità di questa classe si riferiscono a luoghi geografici. Possono essere nomi di città ("New
   York"), paesi ("Italia"), luoghi geografici ("Monte Everest") e altro ancora.

4. **DATE**: Questa classe riguarda le informazioni temporali, come date e orari. Ad esempio, "12 marzo 2022" o "alle
   15:00" sarebbero entità di classe "DATE".

5. **TIME**: Questa classe si concentra sugli orari specifici, come "15:00" o "le 10 del mattino".

6. **MONEY**: Le entità di questa classe rappresentano valute o importi di denaro, ad esempio "10 euro" o "1000
   dollari".

7. **PERCENT**: Questa classe è associata a percentuali, come "20%" o "il 50%".

8. **PRODUCT**: Questa classe può essere utilizzata per nomi di prodotti, marchi o beni. Ad esempio, "iPhone" o "Nike".

9. **EVENT**: Le entità di questa classe si riferiscono a eventi od occasioni specifiche, come "la conferenza annuale"
   o "il festival musicale".

10. **MISC**: Questa classe è utilizzata per altre entità che non rientrano nelle categorie sopra menzionate. Può
    includere una vasta gamma di entità, a seconda delle esigenze dell'applicazione.

È importante notare che le classi di entità possono essere adattate e personalizzate per adattarsi a contesti specifici.
Ad esempio, in un'applicazione medica, potrebbero essere definite classi specifiche per entità come "malattia" o
"trattamento". La scelta delle classi di entità dipende dall'obiettivo dell'analisi e dall'applicazione specifica del
riconoscimento delle entità nominative.

### ACE NER categories

Le ACE NER (Automatic Content Extraction) categories sono un insieme di categorie di entità specifiche utilizzate nel
campo dell'elaborazione del linguaggio naturale (NLP) e dell'information extraction. Queste categorie sono state
sviluppate nell'ambito dell'ACE Program (Automatic Content Extraction Program), un'iniziativa di valutazione dell'
estrazione automatica di informazioni da testi, con l'obiettivo di standardizzare e valutare le capacità dei sistemi di
NER. Le categorie ACE NER includono:

1. **PERSON**: Rappresenta nomi di persone.

2. **ORGANIZATION**: Si riferisce a nomi di organizzazioni o aziende.

3. **LOCATION**: Indica luoghi geografici, come città, paesi o regioni.

4. **FACILITY**: Questa categoria è utilizzata per nomi di strutture fisiche, come aeroporti, stadi o edifici.

5. **GPE (Geopolitical Entity)**: Rappresenta entità geopolitiche, come stati o province.

6. **VEHICLE**: Si riferisce a veicoli, come automobili, aerei o navi.

7. **WEAPON**: Questa categoria identifica armi o dispositivi correlati alla difesa.

8. **DATE**: Indica informazioni temporali, come date e orari.

9. **TIME**: Si concentra sugli orari specifici.

10. **MONEY**: Rappresenta valute o importi di denaro.

11. **PERCENT**: Indica percentuali.

12. **QUANTITY**: Si riferisce a quantità numeriche o misure, come "5 chilogrammi" o "10 metri".

13. **ORDINAL**: Questa categoria è utilizzata per esprimere ordini o posizioni, come "primo" o "secondo".

14. **OTHER**: Utilizzato per altre entità che non rientrano nelle categorie sopra menzionate o per entità ambigue.

Le ACE NER categories sono state progettate per coprire una vasta gamma di tipi di entità che possono essere presenti
nei testi, consentendo ai sistemi di NER di identificarle e classificarle in categorie specifiche. Queste categorie sono
spesso utilizzate in competizioni e valutazioni nell'ambito del NER per valutare le prestazioni dei sistemi di
estrazione automatica di informazioni da testi.

---

## Notazione IOB

[~ Vai all'indice](#indice-degli-argomenti)

La notazione IOB è una convenzione utilizzata nell'elaborazione del linguaggio naturale (NLP) per etichettare sequenze
di parole o token in un testo, in particolare quando si tratta di problemi di riconoscimento delle entità nominate (NER)
e di estrazione di informazioni. IOB è un acronimo che sta per "Inside, Outside, Beginning."

La notazione IOB funziona in questo modo:

1. **B- (Beginning)**: Indica l'inizio di un'entità. Questa etichetta viene assegnata al primo token di un'entità
   nominata nella sequenza di parole. Ad esempio, se stiamo etichettando una persona di nome "John Smith," "John"
   riceverà l'etichetta "B-PERSON," che indica l'inizio dell'entità persona.

2. **I- (Inside)**: Questa etichetta viene utilizzata per tutti i token successivi all'inizio di un'entità nominata. Ad
   esempio, nella sequenza "John Smith," dopo "John," "Smith" riceverà l'etichetta "I-PERSON," che indica che fa parte
   della stessa entità persona.

3. **O- (Outside)**: Questa etichetta viene assegnata a tutti i token che non fanno parte di un'entità. Ad esempio, in
   una frase che non contiene entità nominate, tutti i token riceveranno l'etichetta "O."

Ecco un esempio pratico per illustrare come funziona la notazione IOB in un contesto di NER:

Testo di esempio: "Il presidente Barack Obama ha tenuto un discorso."

- "Il" - O
- "presidente" - O
- "Barack" - B-PERSON
- "Obama" - I-PERSON
- "ha" - O
- "tenuto" - O
- "un" - O
- "discorso" - O

Nell'esempio sopra, notiamo che "Barack" è l'inizio di un'entità persona ("B-PERSON"), mentre "Obama" fa parte della
stessa entità ("I-PERSON"). Gli altri token che non fanno parte di un'entità ricevono l'etichetta "O" per "Outside."

Questa notazione è utile perché consente di rappresentare sequenze di token in modo strutturato e di identificare
facilmente l'inizio e la fine delle entità. È ampiamente utilizzata nella marcatura dei dati per l'addestramento di
modelli di NER e nella valutazione delle prestazioni dei sistemi di riconoscimento delle entità nominate.

---

## Approcci di sequence labeling

[~ Vai all'indice](#indice-degli-argomenti)

Nel contesto del riconoscimento delle entità nominate (NER), gli approcci di sequence labeling sono metodi utilizzati
per assegnare etichette o categorie a ciascun token in una sequenza di parole o token in un testo. Gli approcci di
sequence labeling per la NER si concentrano sulla marcatura delle entità all'interno del testo. Ecco alcuni degli
approcci più comuni:

1. **Approcci basati su regole**: Questi approcci utilizzano regole linguistiche e pattern per identificare entità nelle
   frasi. Ad esempio, potrebbero cercare nomi propri inizianti con lettere maiuscole o sequenze di parole che sembrano
   nomi di organizzazioni.

2. **Approcci basati su dizionari**: Questi approcci utilizzano dizionari di entità nominate (come elenchi di nomi di
   persone, organizzazioni, luoghi, ecc.) per cercare corrispondenze nei testi. Se una parola corrisponde a una voce nel
   dizionario, viene etichettata come entità corrispondente.

3. **Approcci basati su apprendimento automatico**:
    - **Approcci basati su algoritmi di classificazione**: Questi approcci addestrano modelli di classificazione (come
      Support Vector Machines, Decision Trees o Random Forests) per classificare ciascun token in una delle categorie di
      entità (ad esempio, PERSON, ORGANIZATION, LOCATION, ecc.) utilizzando feature estratte dai token stessi e dal
      contesto.
    - **Approcci basati su reti neurali**: Questi approcci utilizzano reti neurali, come le reti neurali ricorrenti
      (RNN) o le reti neurali convoluzionali (CNN), per apprendere direttamente dai dati come etichettare i token. Le
      reti neurali sequenziali, come le Long Short-Term Memory (LSTM) o le reti neurali trasformatori, sono spesso
      utilizzate per modellare sequenze di parole e assegnare etichette di entità.

4. **Approcci basati su trasformatori**: Gli approcci basati su modelli di trasformatori, come BERT (Bidirectional
   Encoder Representations from Transformers) e GPT (Generative Pre-trained Transformer), sono diventati popolari per la
   NER. Questi modelli sono preaddestrati su grandi quantità di testo e possono essere affinati per compiti specifici,
   inclusa la NER.

5. **Approcci ibridi**: Talvolta, vengono utilizzati approcci ibridi che combinano più metodi, ad esempio, utilizzando
   regole o dizionari per migliorare le prestazioni di un modello di apprendimento automatico.

6. **Approcci ensemble**: Gli ensemble combinano diversi modelli o approcci per ottenere risultati migliori. Ad esempio,
   possono essere combinati un modello basato su trasformatori con un modello basato su reti neurali ricorrenti.

7. **Approcci di raffinamento post-processing**: Dopo che un modello ha etichettato la sequenza di token, è possibile
   applicare tecniche di raffinamento post-processing per migliorare la coerenza delle entità estratte. Questo può
   includere la fusione di entità vicine o l'eliminazione di entità sovrapposte.

La scelta dell'approccio dipende spesso dalle dimensioni del dataset, dalla complessità del problema NER, dalle risorse
disponibili e dalla disponibilità di dati annotati. Gli approcci basati su apprendimento automatico e su trasformatori
tendono ad avere prestazioni elevate su dataset di grandi dimensioni, ma gli approcci basati su regole o dizionari
possono essere utili quando si dispone di risorse limitate o per compiti specifici. Inoltre, l'uso di ensemble e
raffinamenti post-processing può migliorare ulteriormente le prestazioni del sistema NER.

### Approcci "feature-based"

Gli approcci "feature-based" nell'ambito del riconoscimento delle entità nominate (NER) sono metodi che si basano sulla
creazione di feature o caratteristiche rilevanti dai token o dalle parole nei testi. Queste feature vengono poi
utilizzate per addestrare modelli di machine learning, come Support Vector Machines (SVM), Decision Trees, o altri
algoritmi di classificazione, per assegnare etichette di entità ai token. Ecco come funzionano gli approcci
"feature-based" e alcuni degli algoritmi utilizzati:

1. **Estrazione delle feature**: In questa fase, vengono estratte feature rilevanti da ciascun token nel testo e dal suo
   contesto. Le feature possono includere informazioni sulla parola stessa (ad esempio, le maiuscole o minuscole), sulla
   sua posizione nella frase, sulle parole circostanti, sulle radici delle parole (lemma), sulle caratteristiche
   linguistiche (ad esempio, parte del discorso), e altre informazioni pertinenti.

2. **Costruzione del set di addestramento**: Le feature estratte vengono utilizzate per creare un set di addestramento,
   che è costituito da esempi di testi annotati, dove a ciascun token è stata assegnata un'etichetta di entità (ad
   esempio, PERSON, ORGANIZATION, ecc.). Questo set di addestramento è essenziale per l'addestramento dei modelli.

3. **Scelta dell'algoritmo di classificazione**: Gli algoritmi di classificazione comunemente utilizzati negli
   approcci "feature-based" per la NER includono:
    - **Support Vector Machines (SVM)**: Questo algoritmo cerca di trovare un iperpiano che massimizza la separazione
      tra le diverse categorie di entità.
    - **Decision Trees**: Questo algoritmo costruisce un albero decisionale basato sulle feature per effettuare la
      classificazione.
    - **Random Forest**: È una versione di ensemble di alberi decisionali, che combina i risultati di più alberi per
      migliorare la precisione.
    - **Conditional Random Fields (CRF)**: Questo modello è progettato specificamente per problemi di sequenza, come la
      NER, e tiene conto delle dipendenze tra le etichette delle sequenze.
    - **Maximum Entropy (MaxEnt)**: Questo algoritmo cerca di trovare la distribuzione di probabilità più equa data una
      serie di vincoli.

4. **Addestramento del modello**: Il modello di classificazione viene addestrato utilizzando il set di addestramento,
   che consiste in esempi di testo con feature estratte e relative etichette di entità.

5. **Classificazione dei token**: Una volta addestrato, il modello può essere utilizzato per classificare i token in un
   testo non annotato, assegnando loro etichette di entità.

6. **Valutazione delle prestazioni**: Le prestazioni del modello vengono valutate utilizzando metriche come la
   precisione, il richiamo e la F1-score su un set di dati di test annotati.

Gli approcci "feature-based" sono stati ampiamente utilizzati nella NER prima dell'avvento dei modelli basati su
trasformatori, come BERT. Tuttavia, sono ancora validi in situazioni in cui non ci sono grandi quantità di dati annotati
e in cui è necessario utilizzare algoritmi di classificazione più tradizionali.

### Gazetteer

I "gazetteers," anche noti come dizionari o liste di gazetteer, sono raccolte di parole o frasi specifiche associate a
categorie o significati particolari. Questi elenchi sono spesso utilizzati nell'elaborazione del linguaggio naturale
(NLP) per scopi diversi, tra cui il riconoscimento delle entità nominate (NER) e altre attività di estrazione di
informazioni. Ecco come funzionano i gazetteers:

1. **Dizionari di parole chiave**: I gazetteers contengono spesso parole chiave o frasi che sono rilevanti per uno
   specifico contesto o dominio. Ad esempio, un gazetteer per il campo della medicina potrebbe contenere una lista di
   nomi di malattie, farmaci o procedure mediche importanti.

2. **Categorizzazione delle entità**: Ogni parola o frase nel gazetteer è associata a una categoria o a un significato
   specifico. Ad esempio, nel gazetteer medico, la parola "aspirina" potrebbe essere associata alla categoria "farmaco."

3. **Utilizzo nell'elaborazione del testo**: Durante il processo di analisi del testo, i gazetteers vengono utilizzati
   per cercare corrispondenze tra le parole o le frasi nel testo e le voci nel dizionario. Quando una corrispondenza
   viene trovata, è possibile assegnare una categoria specifica all'entità corrispondente nel testo.

4. **Integrazione con algoritmi di NLP**: I gazetteers possono essere integrati con algoritmi di NLP più complessi. Ad
   esempio, possono essere utilizzati in combinazione con algoritmi di riconoscimento delle entità nominate (NER) per
   migliorare l'identificazione di entità specifiche. Se una parola nel testo corrisponde a una voce nel gazetteer, può
   essere assegnata un'etichetta di entità appropriata.

5. **Personalizzazione**: I gazetteers possono essere personalizzati per adattarsi a contesti specifici o a esigenze
   particolari. Ad esempio, in un progetto NLP per il settore finanziario, potrebbe essere creato un gazetteer
   contenente nomi di società finanziarie rilevanti.

6. **Aggiornamenti**: I gazetteers devono essere periodicamente aggiornati per rimanere rilevanti, poiché i termini e le
   entità possono cambiare nel tempo. Ad esempio, nuovi farmaci possono essere introdotti nel campo medico, rendendo
   necessario l'aggiornamento del gazetteer corrispondente.

In sintesi, i gazetteers sono strumenti utili nell'elaborazione del linguaggio naturale per identificare entità
specifiche o parole chiave rilevanti in un testo. Sono particolarmente utili in contesti in cui è necessario riconoscere
entità o concetti specifici e possono essere integrati con altri metodi di analisi del testo per migliorare la
precisione e la coerenza delle annotazioni.

### BiLSTM

La BiLSTM, o Bidirectional Long Short-Term Memory, è una variante delle reti neurali ricorrenti (RNN) utilizzata
nell'ambito dell'elaborazione del linguaggio naturale (NLP) per modellare sequenze di dati, come frasi o testi. La
BiLSTM è progettata per catturare le dipendenze a lungo termine nelle sequenze e per tenere conto del contesto sia
precedente che successivo a ciascun elemento della sequenza. Ecco come funziona:

1. **Reti LSTM (Long Short-Term Memory)**: Per capire la BiLSTM, è utile comprendere le reti LSTM di base. Le LSTM sono
   un tipo di rete neurale ricorrente che è stata progettata per gestire il problema della scomparsa del gradiente nelle
   RNN tradizionali. Le LSTM contengono unità di memoria chiamate "celle LSTM" che possono memorizzare informazioni per
   lunghi periodi e decidere quando dimenticare o aggiornare queste informazioni. Ciò le rende particolarmente adatte
   per la modellazione di sequenze complesse.

2. **Bidirectional LSTM**: La principale caratteristica distintiva della BiLSTM è che utilizza due LSTM separate per
   processare la sequenza di dati: una in avanti (forward LSTM) e una all'indietro (backward LSTM). Queste due LSTM
   lavorano contemporaneamente per catturare il contesto sia precedente che successivo a ciascun elemento nella
   sequenza.

3. **Combinazione dei risultati**: Dopo che le due LSTM hanno elaborato la sequenza da entrambe le direzioni, i loro
   risultati vengono combinati in un'unica rappresentazione. Questa rappresentazione tiene conto delle informazioni sia
   passate che future per ciascun elemento nella sequenza. La combinazione può essere fatta in vari modi, ad esempio
   concatenando i vettori di uscita delle LSTM forward e backward.

4. **Applicazioni**: La BiLSTM è ampiamente utilizzata in NLP per una serie di compiti, tra cui il riconoscimento delle
   entità nominate (NER), la traduzione automatica, la generazione di testo, la classificazione di testi e molto altro.
   La sua capacità di catturare il contesto da entrambe le direzioni la rende efficace nell'analizzare il significato e
   la struttura di frasi e testi complessi.

La BiLSTM è stata una delle pietre miliari nell'evoluzione delle architetture di reti neurali per il NLP. Tuttavia, è
importante notare che negli ultimi anni sono emerse architetture ancora più avanzate, come i modelli basati su
trasformatori, che hanno superato le prestazioni delle reti LSTM in molte applicazioni di NLP.

---

## Entity Linking

[~ Vai all'indice](#indice-degli-argomenti)

L'entity linking (o collegamento di entità) è una tecnica nell'ambito dell'elaborazione del linguaggio naturale (NLP)
che ha l'obiettivo di associare le menzioni di entità all'interno di un testo non strutturato a entità specifiche nei
database di conoscenza o nelle risorse ontologiche. Questo processo consente di collegare le menzioni di entità, come
nomi di persone, organizzazioni, luoghi o concetti, alle loro corrispondenti entità del mondo reale o a voci di un
database. Ecco come funziona e alcune delle sue applicazioni:

**Come funziona l'entity linking:**

1. **Identificazione delle menzioni di entità**: La prima fase dell'entity linking consiste nell'identificare
   all'interno del testo le menzioni di entità. Queste menzioni possono essere riconosciute attraverso il riconoscimento
   delle entità nominate (NER), che estrae nomi di persone, organizzazioni, luoghi, ecc., dal testo.

2. **Disambiguazione delle menzioni**: Una volta identificate le menzioni di entità, il sistema deve determinare a quale
   entità del mondo reale o voce del database ciascuna menzione si riferisce. Questo è noto come il problema della
   disambiguazione delle entità. La disambiguazione può essere complessa, poiché una menzione potrebbe corrispondere a
   più entità possibili. Ad esempio, la menzione "Apple" potrebbe riferirsi all'azienda Apple Inc. o al frutto mela.

3. **Corrispondenza con il database**: Per effettuare la disambiguazione, vengono utilizzati database di conoscenza o
   risorse ontologiche, come Wikidata, DBpedia o Wikipedia. Il sistema confronta le menzioni con le voci nel database e
   cerca di trovare una corrispondenza basata sul contesto e sulle informazioni disponibili. L'utilizzo di link tra il
   testo e le voci del database consente di arricchire il significato delle menzioni.

4. **Assegnazione dell'entità**: Una volta identificata la corrispondenza con una voce del database, l'entity linking
   assegna l'entità o la voce appropriata alla menzione nel testo. Ad esempio, la menzione "Apple" può essere collegata
   all'entità "Apple Inc." nel database.

**Applicazioni dell'entity linking:**

L'entity linking ha numerose applicazioni in diversi campi:

1. **Recupero dell'informazione**: Per migliorare la ricerca di informazioni, collegando i concetti menzionati nel testo
   alle risorse di conoscenza, facilitando così la ricerca di documenti o informazioni correlate.

2. **Question Answering**: Nell'ambito dei sistemi di risposta alle domande, l'entity linking aiuta a comprendere e
   rispondere alle domande che fanno riferimento a entità specifiche.

3. **Traduzione automatica**: Nel processo di traduzione automatica, l'entity linking può essere utilizzato per gestire
   le entità che potrebbero non avere una traduzione diretta in altre lingue.

4. **Analisi dei social media**: Per estrarre informazioni utili dai social media, come identificare aziende o luoghi
   menzionati nei tweet o nei post.

5. **Elaborazione del testo biomedico**: Nella ricerca medica, l'entity linking è utilizzato per collegare i termini
   medici alle voci nei database biomedici.

6. **Automazione dell'assistenza clienti**: Per identificare entità menzionate nelle richieste dei clienti e fornire
   risposte più accurate e contestualmente rilevanti.

In generale, l'entity linking è un componente cruciale per comprendere il contesto e arricchire l'analisi del testo,
consentendo ai sistemi di NLP di collegare le informazioni non strutturate a fonti di conoscenza strutturate.

---

## Relation Extraction

[~ Vai all'indice](#indice-degli-argomenti)

La relation extraction (estrazione delle relazioni) è una tecnica nell'ambito dell'elaborazione del linguaggio
naturale (NLP) che si concentra sull'identificazione e l'estrazione di relazioni semantiche tra entità all'interno di
testi non strutturati. L'obiettivo principale della relation extraction è trovare e descrivere le connessioni o le
interazioni tra diverse entità nominative, come persone, organizzazioni, luoghi, concetti, ecc. Questo processo è
importante per estrarre conoscenze strutturate dai testi e per comprendere meglio le relazioni tra le entità nel mondo
reale. Ecco come funziona e alcune delle sue applicazioni:

**Come funziona la relation extraction:**

1. **Identificazione delle entità**: La prima fase della relation extraction è l'identificazione delle entità all'
   interno del testo. Questo processo può essere effettuato utilizzando il riconoscimento delle entità nominate (NER),
   che estrae nomi di entità come soggetti e oggetti delle relazioni.

2. **Identificazione delle relazioni potenziali**: Una volta individuate le entità, il sistema cerca frasi o contesti
   nel testo in cui queste entità sembrano essere correlate o in cui è possibile che esista una relazione. Queste frasi
   o contesti sono chiamati "candidati relazione."

3. **Classificazione delle relazioni**: Per ciascun candidato relazione, il sistema deve determinare quale tipo di
   relazione esiste tra le entità coinvolte. Questo è un compito di classificazione, dove l'obiettivo è assegnare una
   categoria o un tipo specifico di relazione ai candidati. Ad esempio, si potrebbe cercare di stabilire se due entità
   sono "padre" e "figlio," o se sono "azienda" e "sede."

4. **Estrazione delle relazioni**: Una volta assegnate le categorie di relazione ai candidati, si procede con
   l'estrazione delle relazioni effettive. Le relazioni estratte vengono solitamente rappresentate come coppie di entità
   collegate da una relazione specifica. Ad esempio, "(John Smith, padre di, Jane Smith)" potrebbe essere una relazione
   estratta tra due entità.

**Applicazioni della relation extraction:**

La relation extraction ha numerose applicazioni in diversi campi:

1. **Recupero dell'informazione**: Per estrarre informazioni strutturate da documenti non strutturati, facilitando la
   ricerca e l'organizzazione delle informazioni.

2. **Costruzione di basi di conoscenza**: Per costruire basi di conoscenza strutturate od ontologie, che contengono
   relazioni tra entità del mondo reale.

3. **Elaborazione del testo biomedico**: Nella ricerca medica, l'identificazione di relazioni tra proteine, malattie e
   farmaci è essenziale per comprendere le interazioni biologiche.

4. **Monitoraggio delle notizie e delle opinioni**: Per identificare relazioni tra aziende e tendenze di mercato nelle
   notizie finanziarie, o tra persone e opinioni nelle recensioni online.

5. **Analisi delle reti sociali**: Per rilevare relazioni tra individui o entità all'interno di reti sociali, come
   amicizie o collaborazioni professionali.

6. **Traduzione automatica**: Per comprendere il significato di frasi complesse in diversi contesti culturali,
   identificando le relazioni semantiche tra le entità coinvolte.

In sintesi, la relation extraction è un processo chiave per estrarre relazioni significative dai testi, contribuendo a
trasformare informazioni non strutturate in dati strutturati e conoscenza utile per una vasta gamma di applicazioni
nell'ambito del NLP e oltre.

### Approcci di relation extraction

L'estrazione delle relazioni (relation extraction) è una sfida complessa nell'ambito dell'elaborazione del linguaggio
naturale (NLP), e sono stati sviluppati diversi approcci per affrontare questo problema. Ecco alcuni degli approcci
principali utilizzati per la relation extraction:

1. **Approcci basati su regole**: Questi approcci utilizzano regole linguistiche o pattern specifici per identificare
   relazioni tra entità in base a sintassi o contesto. Ad esempio, potrebbero cercare frasi con strutture del tipo "X è
   il fondatore di Y" o "Z è il presidente di A" per estrarre relazioni specifiche.

2. **Approcci basati su dizionari**: Utilizzano dizionari o liste predefinite di relazioni e delle parole chiave per
   cercare corrispondenze nei testi. Se una frase contiene parole o pattern corrispondenti a quelli nel dizionario,
   viene considerata una possibile relazione.

3. **Approcci basati su apprendimento automatico supervisionato**:
    - **Classificazione binaria**: In questo approccio, si tratta la relation extraction come un problema di
      classificazione binaria in cui l'obiettivo è determinare se una coppia di entità è correlata da una relazione
      specifica o meno. Si utilizzano algoritmi di apprendimento automatico, come Support Vector Machines (SVM), Reti
      Neurali, o Decision Trees, per addestrare modelli di classificazione su dati annotati.
    - **Classificazione multi-classe**: Qui, l'obiettivo è classificare le coppie di entità in più classi di relazioni
      diverse, ad esempio, "padre di," "madre di," "con sede in," ecc.

4. **Approcci basati su reti neurali**:
    - **Convolutional Neural Networks (CNN)**: Utilizzano reti neurali convoluzionali per l'estrazione di feature dai
      testi e per la classificazione delle relazioni.
    - **Recurrent Neural Networks (RNN)**: Le reti neurali ricorrenti, come le Long Short-Term Memory (LSTM), possono
      essere utilizzate per la relation extraction, soprattutto quando si tratta di testi di lunghezza variabile.
    - **Reti Neurali Trasformatori**: I modelli basati su trasformatori, come BERT, possono essere affinati per
      l'estrazione delle relazioni sfruttando la loro capacità di comprensione del contesto.

5. **Approcci basati su grafi**: Rappresentano i testi sotto forma di grafi, dove le entità sono nodi e le relazioni
   sono archi. Successivamente, vengono utilizzati algoritmi di estrazione di sottografi o tecniche di grafi per
   estrarre relazioni.

6. **Approcci basati su embedding**: Utilizzano tecniche di embedding, come Word Embeddings o Embeddings di grafi, per
   rappresentare le parole, le entità e le relazioni. Queste rappresentazioni vengono poi utilizzate per trovare
   corrispondenze tra entità e relazioni nei testi.

7. **Approcci basati su trasferimento di conoscenza**: Utilizzano modelli preaddestrati su dati di larga scala, come
   BERT, e li adattano specificamente per l'estrazione delle relazioni su un dominio o un dataset specifico.

8. **Approcci basati su ensemble**: Combinano più modelli o approcci diversi per migliorare le prestazioni complessive
   dell'estrazione delle relazioni.

La scelta dell'approccio dipende spesso dalla natura del problema di relation extraction, dalla quantità di dati
annotati disponibili e dal contesto specifico dell'applicazione. Spesso, una combinazione di più approcci può portare a
risultati migliori, soprattutto in scenari complessi di estrazione delle relazioni.