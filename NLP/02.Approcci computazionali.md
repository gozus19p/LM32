# Approcci computazionali

### Indice degli Argomenti

* [Approcci computazionali](#approcci-computazionali)
    * [Indice degli Argomenti](#indice-degli-argomenti)
    * [Algoritmo e task](#algoritmo-e-task)
    * [GOF-AI (Good Old Fashion AI) - approccio simbolico](#gof-ai-good-old-fashion-ai---approccio-simbolico)
        * [Ambiti di applicazione del GOFAI](#ambiti-di-applicazione-del-gofai)
    * [Statistical Machine Learning](#statistical-machine-learning)
        * [Che cosa sono le "features"](#che-cosa-sono-le-features)
    * [Deep Learning](#deep-learning)
    * [GOFAI, Machine Learning e Deep Learning a confronto](#gofai-machine-learning-e-deep-learning-a-confronto)
    * [Il ruolo dei dati nei sistemi ML e DL](#il-ruolo-dei-dati-nei-sistemi-ml-e-dl)
        * [Divisione in Train, Test e Validation](#divisione-in-train-test-e-validation)
        * [Supervisione](#supervisione)
        * [Annotazione](#annotazione)
        * [Suddivisione dei dati di addestramento](#suddivisione-dei-dati-di-addestramento)
    * [Valutazione di un modello](#valutazione-di-un-modello)
        * [Precision](#precision)
        * [Recall](#recall)
        * [F1-score (Fmeasure)](#f1-score-fmeasure)

---

## Algoritmo e task

La differenza tra un algoritmo e un task è un concetto fondamentale nell'ambito dell'Natural Language Processing (NLP) e
in molte altre aree dell'informatica. Ecco una spiegazione delle due nozioni:

1. **Algoritmo**:
    - Un algoritmo è una sequenza di istruzioni ben definite che risolve un problema o esegue un compito specifico.
    - Gli algoritmi sono generalmente indipendenti dal dominio o dal problema specifico che si sta cercando di
      risolvere. Possono essere applicati in contesti diversi.
    - Gli algoritmi definiscono il "come" fare qualcosa. Specificano una serie di passi che devono essere seguiti per
      raggiungere un obiettivo.

2. **Task**:
    - Un task (compito) è il problema o l'attività specifica che si desidera risolvere o eseguire.
    - I task possono variare ampiamente nell'ambito dell'NLP. Ad esempio, la traduzione automatica, il riconoscimento
      degli intenti in un testo, l'estrazione di informazioni da un testo, la generazione automatica di testo sono tutti
      esempi di task nell'NLP.
    - I task definiscono il "cosa" deve essere fatto, ma spesso richiedono l'uso di algoritmi specifici per essere
      risolti.

In breve, un algoritmo è un metodo generale e astratto per risolvere problemi, mentre un task è il problema specifico
che si vuole risolvere o il compito che si vuole eseguire utilizzando un algoritmo o una serie di algoritmi. Nel
contesto dell'NLP, gli algoritmi vengono utilizzati per risolvere una vasta gamma di task che coinvolgono il linguaggio
naturale.

---

## GOF-AI (Good Old Fashion AI) - approccio simbolico

L'approccio "Good Old Fashioned AI" (GOFAI), noto anche come "simbolico" o "classico", è un paradigma dell'intelligenza
artificiale che si basa principalmente sull'uso di simboli e regole simboliche per rappresentare e manipolare la
conoscenza e l'intelligenza. Questo approccio è stato predominante nei primi anni della ricerca sull'IA, dagli anni '50
agli anni '80, prima dell'avvento del machine learning e dell'approccio basato su dati. Ecco come funziona il GOFAI:

1. **Rappresentazione del Conoscenza**: Nel GOFAI, la conoscenza è rappresentata in modo esplicito attraverso simboli e
   regole. Questi simboli possono rappresentare concetti, fatti o oggetti del mondo reale, mentre le regole definiscono
   come combinare e manipolare questi simboli per effettuare ragionamenti e decisioni. Ad esempio, un fatto come "gli
   uccelli sono animali" potrebbe essere rappresentato come una regola simbolica.

2. **Inferenza Simbolica**: Il GOFAI fa affidamento su inferenze simboliche per ragionare e prendere decisioni. Questo
   significa che utilizza regole di logica formale per derivare nuove informazioni dalla conoscenza esistente. Ad
   esempio, se si sa che "Tweety è un uccello" e "gli uccelli sono animali", l'inferenza simbolica può essere utilizzata
   per concludere che "Tweety è un animale".

3. **Programmazione Manuale**: Nell'approccio GOFAI, gli esperti devono scrivere manualmente le regole e le
   rappresentazioni simboliche. Questo richiede una conoscenza dettagliata del dominio e può essere laborioso e
   suscettibile di errori.

4. **Limiti di Scalabilità**: Una delle sfide principali del GOFAI è la sua limitata capacità di scalare a compiti
   complessi. Creare un insieme completo di regole per rappresentare la conoscenza umana è estremamente difficile, e
   spesso si verifica una "combinatoria esplosiva" quando si tenta di coprire tutti gli scenari possibili.

5. **Applicazioni Strettamente Definite**: Il GOFAI è stato efficace in applicazioni strettamente definite e limitate,
   dove le regole e le rappresentazioni simboliche possono essere facilmente stabilite. Ad esempio, è stato utilizzato
   con successo in sistemi di ragionamento esperto per la diagnosi medica e la pianificazione automatizzata.

Negli ultimi anni, l'intelligenza artificiale è passata sempre più all'uso di approcci basati su dati e apprendimento
automatico, come le reti neurali artificiali, che hanno dimostrato di essere molto più adatte per compiti complessi come
il riconoscimento del linguaggio naturale e la visione artificiale. Tuttavia, il GOFAI rimane un campo di ricerca
rilevante, specialmente quando si tratta di questioni di rappresentazione del conoscenza e ragionamento simbolico.

### Ambiti di applicazione del GOFAI

L'approccio GOFAI (Good Old Fashioned AI) è stato utilizzato in vari settori e applicazioni, sebbene abbia perso parte
della sua rilevanza rispetto agli approcci di intelligenza artificiale più moderni basati sul machine learning. Ecco
alcune aree in cui il GOFAI è stato utilizzato:

1. **Sistemi di Ragionamento Esperto**: Uno dei principali utilizzi del GOFAI è stato nella creazione di sistemi di
   ragionamento esperto. Questi sistemi sono progettati per emulare il ragionamento di un esperto umano in un campo
   specifico. Gli esperti umani definiscono manualmente le regole e le conoscenze che il sistema deve utilizzare per
   prendere decisioni o fare diagnosi. Ad esempio, i sistemi di ragionamento esperto sono stati utilizzati nella
   diagnosi medica, nella pianificazione logistica e nella gestione delle risorse.

2. **Processi di Linguaggio Naturale**: In alcune applicazioni di elaborazione del linguaggio naturale, il GOFAI è stato
   utilizzato per l'analisi semantica e la comprensione del linguaggio. Ad esempio, sono state sviluppate regole
   grammaticali e semantiche per il parsing e l'analisi di testi in linguaggio naturale.

3. **Robotica**: In alcune applicazioni robotiche, il GOFAI è stato utilizzato per il controllo e la pianificazione
   delle azioni dei robot. Ad esempio, nella pianificazione di movimenti robotici complessi o nell'interpretazione di
   comandi verbali per i robot.

4. **Sistemi di Supporto Decisionale**: In contesti aziendali e industriali, il GOFAI è stato utilizzato per la
   creazione di sistemi di supporto decisionale. Questi sistemi aiutano gli utenti a prendere decisioni basate su regole
   e conoscenze specifiche del dominio.

5. **Giochi da Tavolo**: Il GOFAI è stato utilizzato per sviluppare programmi in grado di giocare a giochi da tavolo
   complessi come gli scacchi e il Go. Questi programmi utilizzano algoritmi di ricerca e regole simboliche per prendere
   decisioni durante il gioco.

Tuttavia, è importante notare che negli ultimi anni, l'attenzione nell'ambito dell'intelligenza artificiale si è
spostata verso l'apprendimento automatico, in particolare il deep learning, che ha dimostrato di avere prestazioni
eccezionali in una vasta gamma di compiti, compresa la visione artificiale, il riconoscimento del linguaggio naturale e
altro ancora. Di conseguenza, il GOFAI è diventato meno prevalente, ma continua a essere rilevante in contesti in cui la
rappresentazione simbolica della conoscenza è cruciale e dove le regole possono essere stabilite con precisione.

---

## Statistical Machine Learning

L'approccio "statistical machine learning" nell'elaborazione del linguaggio naturale (NLP) si basa sull'utilizzo di
modelli statistici e algoritmi di machine learning per affrontare problemi legati al linguaggio naturale. Questo
approccio è diventato dominante nell'NLP negli ultimi anni a causa della sua capacità di gestire compiti complessi di
elaborazione del linguaggio. Ecco come funziona:

1. **Rappresentazione dei dati**: Nel machine learning statistico per l'NLP, i dati vengono rappresentati numericamente
   in modo che possano essere elaborati da algoritmi di machine learning. Questa rappresentazione può includere vettori
   di parole (word embeddings) come Word2Vec o GloVe, che convertono le parole in spazi vettoriali, o rappresentazioni
   più complesse come BERT (Bidirectional Encoder Representations from Transformers).

2. **Addestramento del modello**: Per affrontare un determinato compito nell'NLP, ad esempio la classificazione di testi
   o la generazione automatica di testo, viene selezionato un modello di machine learning appropriato. Questo modello
   viene addestrato su un set di dati di addestramento annotato, che contiene esempi del compito che si desidera
   imparare. Durante l'addestramento, il modello ottimizza i suoi parametri per cercare di fare previsioni accurate
   basate sui dati di addestramento.

3. **Apprendimento dei pattern**: I modelli di machine learning statistico imparano i pattern nei dati di addestramento,
   consentendo loro di generalizzare e fare previsioni su nuovi dati. Ad esempio, un modello potrebbe imparare a
   riconoscere le caratteristiche linguistiche associate a una recensione positiva o negativa su un prodotto.

4. **Validazione e valutazione**: Dopo l'addestramento, il modello viene testato su un set di dati di test separato per
   valutarne le prestazioni. Metriche come l'accuratezza, la precisione, il richiamo e l'F1-score vengono spesso
   utilizzate per misurare l'efficacia del modello.

5. **Utilizzo del modello**: Una volta addestrato e validato, il modello può essere utilizzato per eseguire il compito
   desiderato su nuovi dati. Ad esempio, un modello di classificazione del sentiment potrebbe essere utilizzato per
   analizzare il sentiment di recensioni di prodotti in tempo reale.

6. **Raffinamento e aggiornamento**: I modelli di machine learning possono essere raffinati e aggiornati con nuovi dati
   per migliorare le prestazioni nel tempo. Questo processo è noto come "fine-tuning" e può essere importante per
   mantenere la rilevanza del modello.

È importante notare che l'approccio "statistical machine learning" per l'NLP richiede un ampio set di dati di
addestramento annotato e spesso modelli complessi e calcoli intensivi. Tuttavia, ha dimostrato di avere un notevole
successo in una varietà di compiti nell'elaborazione del linguaggio naturale, inclusi la traduzione automatica,
l'analisi del sentiment, l'estrazione di informazioni e molto altro ancora.

### Che cosa sono le "features"

Nel campo dell'elaborazione del linguaggio naturale (NLP) e del machine learning, il termine "features" si riferisce
alle caratteristiche o agli attributi estratti dai dati testuali o linguistici che vengono utilizzati come input per i
modelli di machine learning. Le features rappresentano le informazioni rilevanti che un modello deve considerare per
svolgere un compito specifico nell'ambito dell'NLP. Ecco alcuni esempi di features nell'NLP:

1. **Parole o Token**: Una delle features più comuni nell'NLP è la rappresentazione delle parole o dei token presenti in
   un testo. Queste rappresentazioni possono essere vettori di parole (word embeddings) che catturano il significato e
   le relazioni tra le parole.

2. **N-grammi**: Gli n-grammi sono sequenze contigue di n parole o token. Le features basate sugli n-grammi catturano le
   relazioni tra le parole in una sequenza e possono essere utilizzate per l'analisi del testo.

3. **Part-of-Speech (POS) Tags**: Le POS tags sono etichette grammaticali assegnate a ciascuna parola in una frase (ad
   esempio, nome, verbo, aggettivo, ecc.). Queste etichette possono essere utilizzate come features per compiti come il
   riconoscimento dell'entità nominata o la disambiguazione del significato delle parole.

4. **Caratteristiche sintattiche**: Le features sintattiche includono informazioni sulla struttura grammaticale di una
   frase, come alberi di dipendenza o alberi sintattici. Queste informazioni possono essere utilizzate per comprendere
   le relazioni sintattiche tra le parole.

5. **Frequenza delle parole**: La frequenza con cui una parola appare in un testo può essere utilizzata come feature, ad
   esempio, per l'analisi del sentiment in base alle parole più frequenti.

6. **Caratteristiche semantiche**: Le features semantiche possono rappresentare il significato delle parole o delle
   frasi. Ad esempio, le feature basate su vettori di parole (word embeddings) catturano il significato delle parole in
   uno spazio vettoriale.

7. **Lunghezza del testo**: La lunghezza del testo può essere utilizzata come feature, ad esempio, per compiti di
   classificazione o per stabilire la complessità di una frase.

Le features sono fondamentali per l'addestramento dei modelli di machine learning nell'NLP, poiché forniscono al modello
informazioni strutturate e rilevanti per il compito che deve svolgere. La selezione e l'ingegnerizzazione delle features
giuste possono avere un impatto significativo sulle prestazioni del modello. In molti casi, l'uso di features avanzate e
rappresentazioni linguistiche sofisticate è essenziale per ottenere risultati accurati in compiti complessi dell'NLP.

---

## Deep Learning

L'approccio "deep learning" è una sottocategoria del machine learning che si basa su reti neurali artificiali profonde (
deep neural networks) per l'apprendimento automatico di rappresentazioni complesse dei dati. Questo approccio è
diventato estremamente popolare nell'elaborazione del linguaggio naturale (NLP) e in molti altri campi dell'intelligenza
artificiale. Ecco come funziona:

1. **Architettura delle Reti Neurali Profonde**: Le reti neurali profonde sono costituite da diversi strati (chiamati
   anche "hidden layers") di neuroni artificiali che operano insieme per apprendere rappresentazioni dei dati. Questi
   strati sono chiamati "profondi" perché possono essere costituiti da molti strati intermedi tra l'input e l'output.

2. **Input dei Dati**: L'input dei dati nell'NLP solitamente consiste in sequenze di parole o token, rappresentate come
   vettori di parole (word embeddings) o altre rappresentazioni numeriche. Questi vettori vengono alimentati nella rete
   neurale come dati di input.

3. **Propagazione in Avanti (Forward Propagation)**: Durante la propagazione in avanti, i dati di input vengono passati
   attraverso i vari strati della rete neurale. In ciascun strato, vengono eseguite operazioni matematiche (solitamente
   moltiplicazioni di matrici e funzioni di attivazione) che trasformano i dati.

4. **Apprendimento dei Pesi**: La rete neurale impara dai dati di addestramento aggiustando i pesi delle connessioni tra
   i neuroni. Questo processo è noto come "apprendimento dei pesi" o "addestramento della rete". L'obiettivo è
   minimizzare una funzione di costo che misura la discrepanza tra le previsioni del modello e i dati di addestramento.

5. **Backpropagation**: Per addestrare la rete, si utilizza l'algoritmo di backpropagation, che calcola i gradienti
   della funzione di costo rispetto ai pesi della rete. Questi gradienti vengono utilizzati per aggiornare i pesi in
   modo che il modello migliori le sue previsioni.

6. **Funzione di Attivazione**: Ogni neurone in una rete neurale utilizza una funzione di attivazione per introdurre non
   linearità nel modello. Funzioni comuni includono ReLU (Rectified Linear Unit), sigmoide e tangente iperbolica.

7. **Strutture Specializzate**: Nell'NLP, sono state sviluppate strutture specializzate di reti neurali, come le reti
   neurali ricorrenti (RNN), le reti neurali convoluzionali (CNN) e le reti neurali trasformative (come BERT e GPT), per
   affrontare compiti specifici come il riconoscimento del linguaggio naturale, la traduzione automatica, la generazione
   di testo e molto altro.

8. **Addestramento e Validazione**: La rete viene addestrata su un set di dati di addestramento e poi valutata su un set
   di dati di validazione o di test per valutarne le prestazioni. Metriche come l'accuratezza, la perdita (loss) o altre
   misure specifiche del compito vengono utilizzate per misurare l'efficacia del modello.

9. **Utilizzo del Modello Addestrato**: Una volta addestrata, la rete neurale può essere utilizzata per effettuare
   previsioni su nuovi dati. Ad esempio, un modello di deep learning nell'NLP può essere utilizzato per tradurre una
   frase in un'altra lingua o per generare testo automaticamente.

L'approccio del deep learning è particolarmente potente perché le reti neurali profonde sono in grado di apprendere
rappresentazioni complesse dei dati, senza la necessità di estrazione manuale delle features. Ciò ha portato a
miglioramenti significativi nelle prestazioni in molti compiti nell'NLP, rendendo il deep learning uno degli strumenti
più efficaci in questo campo.

---

## GOFAI, Machine Learning e Deep Learning a confronto

In sintesi, i tre approcci principali menzionati - GOFAI (Good Old Fashioned AI), machine learning statistico e deep
learning - si differenziano principalmente nei seguenti aspetti:

1. **Rappresentazione della Conoscenza**:
    - **GOFAI**: Si basa sulla rappresentazione simbolica della conoscenza mediante simboli e regole esplicite.
    - **Machine Learning Statistico**: Si basa sull'apprendimento statistico da dati annotati senza una rappresentazione
      simbolica esplicita.
    - **Deep Learning**: Si basa su reti neurali profonde che imparano rappresentazioni complesse dei dati dai dati
      stessi.

2. **Complessità delle Feature**:
    - **GOFAI**: Le features sono definite manualmente e possono essere complesse e specifiche del dominio.
    - **Machine Learning Statistico**: Le features sono spesso estratte manualmente, ma possono anche essere apprese
      automaticamente dal modello.
    - **Deep Learning**: Le features sono apprese automaticamente dal modello durante l'addestramento, senza la
      necessità di estrazione manuale.

3. **Scalabilità**:
    - **GOFAI**: Può essere limitato nella scalabilità a problemi complessi a causa della necessità di definire regole e
      simboli per ciascun problema.
    - **Machine Learning Statistico**: È in grado di scalare meglio a compiti complessi grazie all'uso di algoritmi che
      imparano dalle annotazioni dei dati.
    - **Deep Learning**: È altamente scalabile ed è diventato dominante in molte applicazioni grazie alla sua capacità
      di apprendere rappresentazioni complesse da grandi quantità di dati.

4. **Applicazioni Tipiche**:
    - **GOFAI**: È stato ampiamente utilizzato in sistemi di ragionamento esperto e in applicazioni in cui la conoscenza
      simbolica è fondamentale, come la diagnosi medica.
    - **Machine Learning Statistico**: È stato utilizzato per una vasta gamma di compiti nell'NLP, come la
      classificazione del testo e l'analisi del sentiment, ma anche in altri settori come il riconoscimento delle
      immagini.
    - **Deep Learning**: Ha rivoluzionato l'NLP e ha ottenuto successi significativi in applicazioni come la traduzione
      automatica, il riconoscimento del linguaggio naturale e la generazione di testo, ma è ampiamente utilizzato anche
      in visione artificiale, riconoscimento vocale e altro ancora.

In generale, l'approccio migliore dipende dal problema specifico che si sta cercando di risolvere. Mentre il GOFAI è
adatto per problemi che richiedono una conoscenza simbolica esplicita, il machine learning statistico e il deep learning
sono spesso preferiti per problemi complessi in cui le rappresentazioni dei dati possono essere apprese automaticamente
dai dati stessi. La scelta dell'approccio dipenderà dalla natura del problema, dalla disponibilità di dati e dalla
complessità della rappresentazione richiesta.

---

## Il ruolo dei dati nei sistemi ML e DL

Le modalità di addestramento di un modello nel contesto del machine learning sono fondamentali per valutare e
ottimizzare le prestazioni del modello stesso.

### Divisione in Train, Test e Validation

La divisione dei dati in tre set distinti - addestramento (train), validazione (validation) e test (test) - è una
pratica comune per valutare un modello. Ecco come vengono utilizzati questi set:

1. **Train Set (Set di Addestramento)**: Questo è il set di dati utilizzato per addestrare il modello. Contiene
   esempi del compito che si vuole insegnare al modello. Il modello impara dai dati di addestramento ottimizzando i
   suoi parametri per minimizzare l'errore rispetto a questi dati.

2. **Validation Set (Set di Validazione)**: Dopo l'addestramento, il modello viene testato sul set di validazione,
   che contiene dati diversi da quelli di addestramento. Questo set è utilizzato per regolare iperparametri del
   modello (come la velocità di apprendimento o la dimensione del batch) e per monitorare l'andamento delle
   prestazioni durante l'addestramento. Questa fase è cruciale per evitare l'overfitting, ossia un adattamento
   eccessivo del modello ai dati di addestramento.

3. **Test Set (Set di Test)**: Una volta che il modello ha superato la fase di addestramento e di ottimizzazione
   tramite il set di validazione, viene testato su un set di dati completamente separato chiamato set di test.
   Questo set serve a valutare le prestazioni finali del modello su dati non visti in precedenza. È una stima delle
   prestazioni del modello su nuovi dati in situazioni del mondo reale.

### Supervisione

La supervisione è un concetto chiave nel machine learning e si riferisce al fatto che i dati di addestramento
contengono informazioni di supervisione o "etichette" che indicano la risposta desiderata per ciascun esempio. Ad
esempio, se si addestra un modello di classificazione di immagini per distinguere gatti e cani, ogni immagine avrà
un'etichetta che indica se è un gatto o un cane. La supervisione consente al modello di apprendere come fare
previsioni corrette basate sui dati di addestramento.

### Annotazione

L'annotazione è il processo di aggiunta di etichette o annotazioni ai dati. Questo può essere fatto manualmente da
esseri umani (come l'etichettatura di immagini o la trascrizione di testi) o utilizzando algoritmi per estrarre
automaticamente informazioni dai dati (come il riconoscimento del testo in immagini). L'annotazione è spesso una fase
critica nella preparazione dei dati, in quanto fornisce le etichette necessarie per la supervisione del modello.

In sintesi, la divisione dei dati in set di addestramento, validazione e test è essenziale per valutare e ottimizzare i
modelli di machine learning. La supervisione implica l'uso di etichette nei dati di addestramento per insegnare al
modello, mentre l'annotazione è il processo di aggiunta di queste etichette ai dati. Questi concetti sono fondamentali
per la creazione e la valutazione di modelli di machine learning accurati.

### Suddivisione dei dati di addestramento

La suddivisione dei dati di addestramento in set di train, validation e test è una decisione cruciale che può
influenzare significativamente le prestazioni del tuo modello di machine learning. Non esiste una divisione "una taglia
per tutti" che sia ottimale per tutti i casi, ma ci sono alcune linee guida generali che puoi seguire per effettuare
questa suddivisione in modo efficace. Ecco una suddivisione comune, ma flessibile, dei dati:

1. **Set di Train (Addestramento)**:
    - Solitamente, il set di train rappresenta la maggior parte dei tuoi dati, spesso tra il 60% e l'80% dell'intero
      dataset, a seconda della quantità di dati a disposizione. Maggiore è la quantità di dati di addestramento, meglio
      è in genere per l'apprendimento del modello.

2. **Set di Validation (Validazione)**:
    - Il set di validation è utilizzato per ottimizzare gli iperparametri del modello (come la velocità di apprendimento
      o il numero di epoche di addestramento) e per monitorare le prestazioni del modello durante l'addestramento.
      Dovrebbe rappresentare una percentuale più piccola dei dati di addestramento, solitamente tra il 10% e il 20%.

3. **Set di Test (Test)**:
    - Il set di test è riservato per valutare le prestazioni finali del tuo modello dopo che hai completato
      l'addestramento e l'ottimizzazione. Dovrebbe essere separato dai dati di addestramento e di validation e dovrebbe
      rappresentare una percentuale significativa del tuo dataset, solitamente tra il 10% e il 20%.

Ecco alcuni punti da tenere a mente quando suddividi i dati:

- **Casualità**: Assicurati che la suddivisione dei dati sia casuale. Questo significa che i dati in ciascun set (train,
  validation, test) devono essere selezionati in modo casuale dal dataset originale per evitare bias.

- **Dimensione del Dataset**: La dimensione del tuo dataset influenzerà la percentuale effettiva di dati in ciascun set.
  Assicurati che ogni set contenga una quantità rappresentativa di dati per ciascuna classe o categoria, specialmente se
  hai classi sbilanciate.

- **Validazione Incrociata**: In alternativa, puoi utilizzare la validazione incrociata (cross-validation) per
  ottimizzare gli iperparametri e valutare le prestazioni del modello. Questo approccio può essere utile quando hai un
  dataset limitato.

- **Monitoraggio delle Prestazioni**: Assicurati di monitorare attentamente le prestazioni del tuo modello sia sul set
  di validation che su quello di test. Se noti un peggioramento delle prestazioni sul set di test rispetto a quello di
  validation, potrebbe essere un segnale di overfitting.

In generale, la chiave è mantenere una suddivisione dei dati che consenta di allenare il modello in modo efficace, di
ottimizzarlo senza contaminare i dati di test e di valutarne le prestazioni in modo accurato su dati non visti. La
suddivisione esatta dipenderà dalla natura del tuo problema e dalla quantità di dati disponibili.

---

## Valutazione di un modello

La "valutazione" di un modello in machine learning è il processo di misurare le prestazioni e l'efficacia di un modello
sui dati di test o su un set di dati separato. L'obiettivo della valutazione è capire quanto bene il modello è in grado
di fare previsioni accurate su dati che non ha mai visto durante l'addestramento. Ci sono diverse metriche utilizzate
per misurare le prestazioni di un modello, tra cui "precision", "recall" e "F1-score", che sono comunemente utilizzate
in problemi di classificazione binaria e multiclasse. Ecco cosa rappresentano queste metriche:

### Precision

1. **Precision** (Precisione):
    - La precisione è una misura della precisione delle previsioni positive fatte dal modello. Si calcola come il
      rapporto tra il numero di veri positivi (esempi positivi correttamente classificati) e la somma di veri positivi e
      falsi positivi (esempi negativi erroneamente classificati come positivi).
    - Formula: Precision = TP / (TP + FP)
    - La precisione è importante quando si desidera minimizzare i falsi positivi, ossia quando fare una previsione
      positiva erroneamente ha conseguenze costose o indesiderate.

### Recall

2. **Recall** (Richiamo o Sensibilità):
    - Il recall è una misura della capacità del modello di individuare tutti gli esempi positivi. Si calcola come il
      rapporto tra il numero di veri positivi (esempi positivi correttamente classificati) e la somma di veri positivi e
      falsi negativi (esempi positivi erroneamente classificati come negativi).
    - Formula: Recall = TP / (TP + FN)
    - Il recall è importante quando è cruciale catturare tutti gli esempi positivi, ad esempio nel riconoscimento di
      malattie o nella sicurezza.

### F1-score (Fmeasure)

3. **F1-score**:
    - L'F1-score è una misura che tiene conto sia della precisione che del recall. È la media armonica tra precisione e
      recall e fornisce una singola misura delle prestazioni del modello.
    - Formula: F1-score = 2 * (Precision * Recall) / (Precision + Recall)
    - L'F1-score è utile quando si desidera bilanciare precisione e recall. È particolarmente utile in situazioni in cui
      ci sono squilibri tra classi.

Queste metriche sono spesso utilizzate insieme per ottenere una visione completa delle prestazioni di un modello di
classificazione. Tuttavia, a seconda del contesto e degli obiettivi del problema, potresti scegliere di concentrarti su
una di queste metriche a scapito delle altre, a seconda delle tue priorità. La scelta delle metriche di valutazione
dipenderà dalla natura del problema e dalle conseguenze delle previsioni errate.