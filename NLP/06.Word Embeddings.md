# Word Embeddings

### Indice degli argomenti

* [Word Embeddings](#word-embeddings)
    * [Indice degli argomenti](#indice-degli-argomenti)
    * [Vettori sparsi vs vettori densi](#vettori-sparsi-vs-vettori-densi)
    * [Word2Vec](#word2vec)
        * [Layer](#layer)
            * [CBOW](#cbow)
            * [Skip-gram](#skip-gram)
        * [Parole mai viste](#parole-mai-viste)
    * [CNN: Convolutional Neural Networks](#cnn-convolutional-neural-networks)
        * [Mappa delle caratteristiche](#mappa-delle-caratteristiche)
    * [RNN: Recurrent Neural Networks](#rnn-recurrent-neural-networks)
    * [Attention](#attention)
        * [Calcolo dei pesi di attention](#calcolo-dei-pesi-di-attention)
    * [Contextualized embeddings](#contextualized-embeddings)
    * [WordPiece tokenization](#wordpiece-tokenization)
    * [MLM: Masked Language Model](#mlm-masked-language-model)

---

## Vettori sparsi vs vettori densi

[~ Vai all'indice](#indice-degli-argomenti)

**Vettori Densi (Dense Vectors)**

Dimensioni Complete: In un word embedding denso, ogni parola è rappresentata da un vettore di dimensioni complete. Ad
esempio, se stiamo utilizzando un embedding di dimensione 300, ogni parola avrà un vettore 300-dimensionale.
Valori Numerici: I vettori densi contengono valori numerici in tutte le loro dimensioni. Questi valori possono essere
decimali e possono assumere qualsiasi valore all'interno della dimensione specifica.
Spazio di Memoria: Poiché ogni parola ha un vettore completo, gli embedding densi richiedono più spazio di memoria
rispetto agli embedding sparsi, specialmente se il vocabolario è grande.
Esempi: Word2Vec e GloVe sono esempi di tecniche che generano embedding densi.

**Vettori Sparsi (Sparse Vectors)**

Dimensioni Parziali: In un word embedding sparso, le parole sono rappresentate da vettori in cui la maggior parte delle
dimensioni è posta a zero (sparse), con solo alcune dimensioni contenenti valori diversi da zero.
Efficienza di Memoria: Gli embedding sparsi richiedono notevolmente meno spazio di memoria rispetto agli embedding
densi, poiché gran parte delle dimensioni è zero.
Esempi: FastText è un esempio di tecnica che può generare embedding sparsi, in quanto tiene conto delle sotto-parole e
utilizza una rappresentazione sparsa per il vocabolario.

In sintesi, la principale differenza tra vettori densi e sparsi nei word embedding sta nella densità dei valori nelle
dimensioni del vettore. I vettori densi sono caratterizzati da valori numerici in tutte le dimensioni, mentre i vettori
sparsi hanno solo alcune dimensioni con valori non nulli. La scelta tra vettori densi e sparsi dipende spesso dalle
esigenze specifiche dell'applicazione e dalle risorse disponibili, in quanto gli embedding densi possono richiedere più
spazio di memoria, ma possono essere più espressivi, mentre gli embedding sparsi possono essere più efficienti in
termini di memoria ma meno espressivi.

---

## Word2Vec

[~ Vai all'indice](#indice-degli-argomenti)

Word2Vec è una tecnica di word embedding ampiamente utilizzata nel campo del Natural Language Processing (NLP). È stata
sviluppata da Tomas Mikolov e il suo team presso Google nel 2013 ed è diventata una delle tecniche più popolari per la
rappresentazione semantica delle parole in un formato numerico. Word2Vec è noto per la sua capacità di catturare le
relazioni semantiche tra le parole in un modo che rende possibile l'analisi matematica delle parole in spazi vettoriali.

Ecco alcuni punti chiave su Word2Vec:

**Obiettivo Principale**: L'obiettivo principale di Word2Vec è quello di convertire parole in vettori numerici in modo
che
parole simili dal punto di vista semantico abbiano rappresentazioni vettoriali simili. In questo modo, è possibile
eseguire operazioni algebriche sui vettori per scoprire relazioni tra le parole.

**Due Architetture**: Word2Vec presenta due architetture principali: CBOW (Continuous Bag of Words) e Skip-gram. CBOW
cerca
di prevedere una parola target dato un contesto di parole circostanti, mentre Skip-gram cerca di prevedere il contesto
di parole circostanti dato una parola target.

**Rappresentazione Vettoriale**: Le parole vengono rappresentate come vettori numerici in uno spazio multidimensionale.
La
dimensione di questi vettori è un parametro specificato dall'utente.

**Similarità Semantica**: Un vantaggio chiave di Word2Vec è che i vettori risultanti catturano le somiglianze semantiche
tra
le parole. Ad esempio, le parole "re" e "regina" avranno vettori più simili tra loro rispetto alle parole "re" e "cane".

**Operazioni Vettoriali**: Poiché le parole sono rappresentate come vettori, è possibile eseguire operazioni matematiche
su
di esse. Ad esempio, è possibile sommare il vettore "regina" al vettore "uomo" e sottrarre il vettore "re" per ottenere
un vettore che rappresenta concettualmente "donna".

**Applicazioni**: Word2Vec è stato ampiamente utilizzato in applicazioni NLP, tra cui la ricerca di documenti simili,
traduzione automatica, raggruppamento di documenti, analisi dei sentimenti e molto altro.

Word2Vec ha contribuito in modo significativo all'avanzamento delle applicazioni basate su NLP, poiché fornisce una
rappresentazione vettoriale efficace delle parole che aiuta a catturare le relazioni semantiche tra di esse.

### Layer

Word2Vec non ha layer nel senso tradizionale che si trova in reti neurali profonde (Deep Neural Networks o DNN).
Word2Vec è una tecnica di word embedding che utilizza due diverse architetture, ma queste architetture non sono
organizzate in layer profondi come in una rete neurale convoluzionale o una rete neurale ricorrente.

Le due architetture principali in Word2Vec sono:

1. **CBOW (Continuous Bag of Words)**: In questa architettura, non ci sono layer nel senso tradizionale delle reti
   neurali. CBOW prende una finestra di contesto di parole circostanti (ad esempio, le parole precedenti e successive a
   una parola target) e cerca di prevedere la parola target. Ci sono pesi associati alle parole nel vocabolario, ma non
   ci sono layer di neuroni in cascata come nelle DNN.

2. **Skip-gram**: Anche nell'architettura Skip-gram, non ci sono layer profondi. Skip-gram prende una parola target e
   cerca di prevedere le parole del contesto circostante. Anche qui, ci sono pesi associati alle parole nel vocabolario,
   ma non ci sono layer nascosti o profondi.

In entrambe queste architetture, l'obiettivo principale è addestrare vettori di parole (word vectors) in modo che
catturino le relazioni semantiche tra le parole, ma ciò avviene attraverso una semplice operazione di apprendimento dei
pesi associati alle parole nel vocabolario, senza la complessità dei layer di neuroni multi-strato delle reti neurali
profonde.

Quindi, in sintesi, Word2Vec non ha layer nel senso delle reti neurali profonde; è una tecnica di apprendimento dei
vettori di parole basata su reti neurali superficiali con un obiettivo specifico di word embedding.

#### CBOW

L'architettura CBOW (Continuous Bag of Words) è una delle due principali architetture utilizzate in Word2Vec per la
creazione di word embedding. La sua funzione principale è quella di prevedere una parola target basandosi su un contesto
di parole circostanti. Ecco come funziona l'architettura CBOW:

1. **Raccolta del Contesto**:
    - L'input per l'architettura CBOW è una finestra di parole circostanti a una parola target all'interno di una frase.
      La dimensione di questa finestra è un parametro specificato dall'utente e determina quante parole circostanti
      vengono considerate.
    - Ad esempio, se consideriamo la frase "Il gatto mangia il pesce" e impostiamo una finestra di dimensione 2,
      possiamo prendere le parole "Il", "gatto", "il", "pesce" come contesto per prevedere la parola target "mangia".

2. **Rappresentazione Vettoriale del Contesto**:
    - Ciascuna delle parole nel contesto viene convertita in una rappresentazione vettoriale, nota come word vector.
      Questi vettori sono ottenuti dai vettori di parole del vocabolario. Ad esempio, "gatto" potrebbe essere
      rappresentato come un vettore numerico.
    - Gli word vector del contesto vengono quindi combinati (di solito attraverso la media) per ottenere una
      rappresentazione numerica complessiva del contesto.

3. **Predizione della Parola Target**:
    - L'obiettivo principale di CBOW è prevedere la parola target sulla base della rappresentazione numerica del
      contesto.
    - La parola target è rappresentata anche come un vettore numerico dal vocabolario. L'obiettivo dell'addestramento è
      minimizzare la differenza tra il vettore previsto e il vettore reale della parola target.
    - La funzione di costo più comune utilizzata per l'addestramento di CBOW è la cross-entropy.

4. **Apprendimento dei Word Vector**:
    - Durante l'addestramento, i pesi associati ai word vector vengono aggiornati iterativamente in modo che il modello
      migliori la sua capacità di prevedere la parola target a partire dal contesto.
    - I word vector vengono regolarmente aggiornati in modo che le parole con contesti simili abbiano rappresentazioni
      vettoriali simili alla fine dell'addestramento.

5. **Risultato Finale**:
    - Alla fine dell'addestramento, otteniamo word vector per tutte le parole nel vocabolario, e questi vettori
      catturano relazioni semantiche tra le parole.

L'architettura CBOW è nota per essere più veloce nell'addestramento rispetto all'architettura Skip-gram, ma potrebbe
catturare meglio le informazioni sulla co-occorrenza delle parole. Tuttavia, la scelta tra CBOW e Skip-gram dipende
dalle specifiche esigenze dell'applicazione.

#### Skip-gram

L'architettura "skip-gram" è un'altra delle due principali architetture utilizzate in Word2Vec per la creazione di word
embedding. A differenza di CBOW, che cerca di prevedere una parola target basandosi su un contesto di parole
circostanti, skip-gram fa il contrario: cerca di prevedere il contesto di parole circostanti dato una parola target.
Ecco come funziona l'architettura "skip-gram":

1. **Selezione della Parola Target**:
    - L'input per l'architettura skip-gram è una singola parola target all'interno di una frase. Questa parola target
      viene selezionata casualmente dalla frase.

2. **Rappresentazione Vettoriale della Parola Target**:
    - La parola target è rappresentata come un vettore numerico dal vocabolario. Questo vettore è l'obiettivo dell'
      addestramento e rappresenta la parola stessa.

3. **Predizione del Contesto**:
    - L'obiettivo principale di skip-gram è prevedere il contesto di parole circostanti alla parola target. Il contesto
      può essere definito da una finestra di parole circostanti, simile a quanto fatto in CBOW.
    - Ad esempio, se la parola target è "mangia" e stiamo considerando una finestra di dimensione 2, l'obiettivo sarà
      prevedere le parole "Il", "gatto", "il", "pesce" che circondano "mangia".

4. **Rappresentazione Vettoriale del Contesto**:
    - Le parole del contesto vengono rappresentate come vettori numerici dal vocabolario, proprio come nella fase di
      CBOW.

5. **Apprendimento dei Word Vector**:
    - Durante l'addestramento, i pesi associati ai word vector vengono aggiornati in modo che il modello migliori la sua
      capacità di prevedere il contesto a partire dalla parola target.
    - Questo significa che i vettori delle parole target nel vocabolario vengono regolarmente aggiornati per catturare
      meglio le relazioni semantiche con le parole circostanti.

6. **Risultato Finale**:
    - Alla fine dell'addestramento, otteniamo word vector per tutte le parole nel vocabolario. Questi vettori catturano
      relazioni semantiche tra le parole e possono essere utilizzati per varie applicazioni di NLP.

L'architettura skip-gram è più flessibile rispetto a CBOW e può catturare relazioni più dettagliate tra le parole, ma
può richiedere più dati di addestramento e più tempo per convergere. La scelta tra skip-gram e CBOW dipende dalle
esigenze specifiche dell'applicazione e dalle dimensioni del dataset a disposizione.

### Parole mai viste

Nel CBOW (Continuous Bag of Words) e nell'architettura Skip-gram, il trattamento delle parole mai viste, ossia parole
che non sono presenti nel vocabolario utilizzato durante l'addestramento, può variare a seconda dell'implementazione
specifica e delle scelte dell'utente. Tuttavia, di solito, le parole mai viste vengono gestite in modo simile nei due
approcci. Ecco come vengono affrontate solitamente:

**1. Ignorare le Parole Mai Viste:**

- La strategia più comune è quella di ignorare le parole mai viste durante l'addestramento e l'utilizzo del modello.
  Questo significa che il modello non ha alcuna rappresentazione vettoriale per queste parole e non può effettuare
  previsioni o calcoli basati su di esse.
- Quando una parola mai vista appare durante l'utilizzo del modello, essa viene trattata come sconosciuta e potrebbe
  ricevere un'etichetta speciale, come "\<UNK\>" (unknown), o essere semplicemente omessa dall'elaborazione.

**2. Sottocampionamento delle Parole Comuni:**

- Per gestire meglio il disavanzo di parole mai viste, è comune applicare una tecnica chiamata sottocampionamento (
  subsampling) delle parole comuni durante l'addestramento. Questo significa che parole molto frequenti vengono ridotte
  in modo casuale per ridurre la loro influenza e dare più importanza alle parole meno comuni.
- Tuttavia, questa tecnica è spesso utilizzata solo con parole comuni. Le parole mai viste, essendo già rare, non sono
  sottocampionate ulteriormente.

**3. Trattamento in Applicazioni Specifiche:**

- A seconda dell'applicazione, le parole mai viste possono essere gestite in modi diversi. Ad esempio, in
  un'applicazione di traduzione automatica, una parola mai vista potrebbe essere tradotta come "sconosciuto" o "parola
  non tradotta" nel testo di destinazione.
- In altre applicazioni, come il raggruppamento di documenti o la ricerca, le parole mai viste possono essere
  semplicemente ignorate, poiché il loro contributo alla comprensione complessiva del testo potrebbe essere limitato.

In generale, il trattamento delle parole mai viste è una sfida comune nell'elaborazione del linguaggio naturale, e la
scelta di come gestirle dipende dall'obiettivo dell'applicazione e dalle strategie specifiche utilizzate durante
l'addestramento del modello.

---

## CNN: Convolutional Neural Networks

[~ Vai all'indice](#indice-degli-argomenti)

Le reti neurali convoluzionali (CNN), o Convolutional Neural Networks in inglese, sono un tipo di architettura di rete
neurale profonda comunemente utilizzate nell'ambito del computer vision e dell'elaborazione di immagini. Queste reti
sono progettate per riconoscere modelli e caratteristiche nelle immagini in modo simile a come il cervello umano elabora
informazioni visive. Ecco come funzionano le CNN:

1. **Convoluzione**:
    - La convoluzione è l'operazione chiave nelle CNN. Consiste nel passaggio di un filtro o kernel su un'immagine per
      estrarre caratteristiche rilevanti. Il filtro è una piccola matrice numerica che scivola sull'immagine pixel per
      pixel e produce una mappa delle caratteristiche (feature map).
    - L'idea è che il filtro impari automaticamente a rilevare pattern come bordi, curve o altre strutture rilevanti
      nell'immagine.

2. **Strato di Convoluzione**:
    - Una CNN è composta da uno o più strati di convoluzione. Ogni strato può avere diversi filtri che estraggono
      diverse caratteristiche.
    - Mentre gli strati iniziali estraggono dettagli di basso livello come bordi e linee, gli strati successivi
      estraggono caratteristiche di livello superiore basate su quelle estratte negli strati precedenti.

3. **Pooling (Sottocampionamento)**:
    - Dopo uno strato di convoluzione, spesso viene applicato uno strato di pooling. Il pooling riduce la dimensione
      della mappa delle caratteristiche, mantenendo le caratteristiche più rilevanti.
    - Il pooling può essere eseguito attraverso operazioni come il max-pooling, che seleziona il valore massimo in una
      regione, o il mean-pooling, che calcola la media dei valori.

4. **Strato Fully Connected (Dense)**:
    - Dopo uno o più strati di convoluzione e pooling, solitamente seguono uno o più strati completamente connessi (
      dense). Questi strati sono simili a quelli delle reti neurali tradizionali e sono utilizzati per l'output finale
      della CNN.
    - Gli strati fully connected collegano tutte le caratteristiche estratte ai neuroni dell'output, che producono la
      classificazione o la previsione desiderata.

5. **Funzione di Attivazione**:
    - In ciascun neurone di una CNN (compreso quelli degli strati fully connected), viene applicata una funzione di
      attivazione, spesso la funzione ReLU (Rectified Linear Unit). Questa funzione introduce la non linearità nella
      rete, rendendo possibile l'apprendimento di relazioni complesse tra le caratteristiche.

6. **Apprendimento e Backpropagation**:
    - Le CNN apprendono dai dati mediante il processo di backpropagation. Durante l'addestramento, la rete cerca di
      minimizzare una funzione di costo confrontando le previsioni con i risultati desiderati.
    - Gli algoritmi di ottimizzazione vengono utilizzati per aggiornare i pesi dei filtri e dei neuroni in modo da
      migliorare le prestazioni della rete.

In sintesi, le CNN utilizzano la convoluzione per estrarre caratteristiche significative dalle immagini e
successivamente passano attraverso strati fully connected per effettuare la classificazione o altre attività desiderate.
Grazie alla loro capacità di apprendere gerarchie complesse di caratteristiche, le CNN sono diventate fondamentali in
applicazioni di visione artificiale come il riconoscimento di oggetti, la classificazione di immagini, la segmentazione
dell'immagine e molto altro.

### Mappa delle caratteristiche

La mappa delle caratteristiche (feature map) è un concetto chiave nelle reti neurali convoluzionali (CNN). Si tratta di
un'immagine tridimensionale o un volume di dati che rappresenta le caratteristiche estratte da uno strato di
convoluzione della rete. Ogni strato di convoluzione produce una serie di mappe delle caratteristiche, ognuna delle
quali rappresenta le risposte dei filtri applicati all'immagine di input. Ecco alcune informazioni importanti sulla
mappa delle caratteristiche:

1. **Dimensioni**: Una mappa delle caratteristiche ha dimensioni in larghezza, altezza e profondità (depth). La
   larghezza e l'altezza corrispondono alle dimensioni spaziali dell'immagine, mentre la profondità rappresenta il
   numero di filtri o kernel utilizzati nello strato di convoluzione.

2. **Estrazione delle Caratteristiche**: Ogni pixel in una mappa delle caratteristiche rappresenta una caratteristica
   specifica rilevata dall'applicazione del filtro corrispondente all'immagine di input. Queste caratteristiche possono
   essere semplici, come bordi, o più complesse, come curve o texture.

3. **Mappa per Filtro**: Ogni filtro nello strato di convoluzione produce una mappa delle caratteristiche separata. Ciò
   significa che per ogni filtro applicato all'immagine di input, otteniamo una mappa delle caratteristiche
   corrispondente.

4. **Stack di Mappe**: All'interno di uno strato di convoluzione, le mappe delle caratteristiche vengono organizzate in
   uno stack tridimensionale. Questo stack rappresenta la collezione di tutte le mappe delle caratteristiche generate da
   tutti i filtri nello strato.

5. **Riduzione Dimensionale**: Tipicamente, le dimensioni spaziali delle mappe delle caratteristiche possono essere
   ridotte attraverso l'operazione di pooling, che aiuta a mantenere le caratteristiche più rilevanti e a ridurre la
   dimensione dei dati da passare agli strati successivi della rete.

6. **Utilizzo nell'Apprendimento**: Le mappe delle caratteristiche estratte da uno strato di convoluzione vengono
   utilizzate come input per gli strati successivi della rete, come gli strati fully connected, che effettuano la
   classificazione o altre attività desiderate.

7. **Gerarchia di Caratteristiche**: Nelle CNN, gli strati di convoluzione sono organizzati in modo gerarchico, con gli
   strati iniziali che estraggono dettagli di basso livello come bordi e linee, e gli strati successivi che combinano
   queste caratteristiche per estrarre concetti di livello superiore, come forme o oggetti.

In sintesi, una mappa delle caratteristiche è una rappresentazione tridimensionale delle informazioni estratte
dall'immagine di input mediante l'applicazione dei filtri di convoluzione. Queste mappe sono fondamentali per la
capacità delle CNN di apprendere e riconoscere modelli e caratteristiche complesse nelle immagini.

---

## RNN: Recurrent Neural Networks

[~ Vai all'indice](#indice-degli-argomenti)

Una RNN, o Rete Neurale Ricorrente (Recurrent Neural Network), è un tipo di rete neurale artificiale progettata per
elaborare dati sequenziali o dati che hanno una struttura temporale, come il linguaggio naturale, il riconoscimento del
parlato, il video, la musica e molti altri. Le RNN sono particolarmente adatte per modellare sequenze a lungo termine
perché hanno una struttura che consente di considerare l'input attuale insieme alle informazioni precedentemente
elaborati.

Ecco come funziona una RNN:

1. **Architettura Ricorrente**:
    - La caratteristica principale di una RNN è la presenza di uno o più strati ricorrenti. Ogni strato ricorrente
      contiene una serie di unità ricorrenti (neuroni) che formano connessioni ricorrenti tra loro.
    - Queste connessioni permettono alle informazioni di essere passate avanti nel tempo, consentendo alla rete di
      memorizzare e utilizzare le informazioni precedentemente elaborate.

2. **Stati Nascosti**:
    - Ogni unità ricorrente ha uno stato nascosto (hidden state) che funge da memoria interna. Questo stato nascosto
      contiene informazioni sulle sequenze precedenti di input ed è utilizzato per influenzare l'elaborazione dell'input
      corrente.
    - L'aggiornamento dello stato nascosto avviene iterativamente man mano che la rete elabora ciascun elemento
      sequenziale.

3. **Passaggio di Informazioni**:
    - Durante l'elaborazione di una sequenza, le RNN ricevono un elemento di input alla volta. L'input corrente viene
      combinato con lo stato nascosto precedente per produrre un output e un nuovo stato nascosto.
    - Questo nuovo stato nascosto contiene informazioni sia sull'input corrente che su ciò che è stato osservato in
      precedenza nella sequenza.

4. **Apprendimento delle Connessioni**:
    - Come in altre reti neurali, le RNN apprendono pesi (connessioni sinaptiche) attraverso l'addestramento.
      L'apprendimento si basa sull'ottimizzazione di una funzione di costo che misura la discrepanza tra le previsioni
      della rete e gli obiettivi desiderati.

5. **Backpropagation Through Time (BPTT)**:
    - Per addestrare le RNN, viene utilizzato un algoritmo chiamato "Backpropagation Through Time" (BPTT). Questo
      algoritmo calcola gradienti di errore rispetto ai pesi della rete e li utilizza per aggiornare i pesi durante
      l'addestramento.
    - La particolarità del BPTT rispetto al backpropagation standard è che tiene conto delle connessioni ricorrenti e
      della dipendenza temporale delle sequenze.

6. **Applicazioni**:
    - Le RNN sono utilizzate in una varietà di applicazioni che coinvolgono dati sequenziali, come traduzione
      automatica, previsione del testo, riconoscimento del parlato, analisi temporale delle serie storiche, e molto
      altro.

Una sfida comune nelle RNN è il problema della "scomparsa del gradiente" (vanishing gradient), in cui gli aggiornamenti
dei pesi diventano troppo piccoli quando la rete deve considerare informazioni a lungo termine. Per mitigare questo
problema, sono state sviluppate varianti di RNN, come le Long Short-Term Memory (LSTM) e le Gated Recurrent Unit (GRU),
che sono in grado di gestire più efficacemente le dipendenze a lungo termine nelle sequenze.

---

## Attention

[~ Vai all'indice](#indice-degli-argomenti)

L'attention, nell'ambito del deep learning e del Natural Language Processing (NLP), è un meccanismo che consente ai
modelli di concentrarsi su parti specifiche di un input quando producono un output. Questo meccanismo è ispirato dalla
capacità umana di concentrarsi su dettagli rilevanti mentre si eseguono compiti complessi.

L'attention ha una grande importanza in applicazioni come la traduzione automatica, il riconoscimento di oggetti in
immagini, la generazione di testi e altro ancora. La sua principale funzione è quella di migliorare la capacità del
modello di selezionare le informazioni più rilevanti da una grande quantità di dati di input.

Ecco come funziona generalmente il meccanismo di attention:

1. **Calcolo dei Pesi di Attention**:
    - Inizialmente, il modello calcola dei pesi di attention per ciascun elemento dell'input. Questi pesi indicano
      l'importanza relativa di ciascun elemento rispetto al compito in corso.
    - I pesi di attention sono spesso calcolati utilizzando funzioni di scoring, che misurano quanto bene ciascun
      elemento si adatta alle esigenze del compito. Una funzione comune è la funzione softmax.

2. **Pesatura dell'Input**:
    - Una volta calcolati i pesi di attention, il modello li utilizza per pesare ciascun elemento dell'input. Gli
      elementi con pesi più alti ricevono maggiore attenzione, mentre quelli con pesi più bassi ricevono meno
      attenzione.

3. **Creazione dell'Output**:
    - L'input pesato con l'attention viene quindi utilizzato per generare l'output desiderato. Ad esempio, in una rete
      neurale tradizionale, questo input pesato potrebbe essere combinato attraverso strati fully connected per produrre
      l'output.

4. **Iterazione**:
    - In molte applicazioni, il processo di attention viene ripetuto più volte, consentendo al modello di focalizzare
      l'attenzione su diverse parti dell'input durante diverse fasi dell'elaborazione.
    - Questo è particolarmente utile per compiti complessi in cui è necessario considerare diverse parti dell'input in
      momenti diversi.

L'attention può essere utilizzata in diverse varianti e modelli. Due delle più famose sono l'Attention Seq2Seq e il
Transformer. Nel caso del Transformer, il meccanismo di attention è estremamente potente ed è alla base dei recenti
progressi nell'elaborazione del linguaggio naturale. Il Transformer utilizza l'attention sia per l'input che per
l'output, consentendo di gestire sequenze di lunghezza variabile e di catturare relazioni complesse tra le parole in
modo altamente parallelo.

### Calcolo dei pesi di attention

Il calcolo dei pesi nell'attention è una parte fondamentale del meccanismo di attention e viene utilizzato per
determinare quanto peso dare a ciascun elemento dell'input quando si produce l'output desiderato. Esistono diverse
varianti del calcolo dei pesi di attention, ma una delle più comuni è l'utilizzo della funzione softmax per ottenere
pesi normalizzati. Ecco come funziona tipicamente il calcolo dei pesi nell'attention:

1. **Calcolo del Punteggio di Attention**:
    - Per ciascun elemento dell'input, viene calcolato un punteggio di attention che misura quanto bene quell'elemento
      si adatta alle esigenze del compito. Il punteggio di attention viene solitamente calcolato utilizzando una
      funzione di scoring, che prende in input l'elemento dell'input e il contesto corrente e restituisce un valore
      numerico.
    - La funzione di scoring può variare a seconda del modello e dell'applicazione. Una delle varianti più comuni è
      l'utilizzo di prodotti scalari tra il vettore di query (che rappresenta l'output desiderato in quel momento) e il
      vettore di chiave (che rappresenta l'elemento dell'input).

2. **Normalizzazione dei Punteggi**:
    - I punteggi di attention ottenuti per tutti gli elementi dell'input vengono solitamente normalizzati attraverso la
      funzione softmax. La funzione softmax converte i punteggi in una distribuzione di probabilità, in modo che la
      somma di tutti i pesi sia uguale a 1.
    - Questo passaggio è importante perché rende i pesi di attention interpretabili come probabilità relative
      all'importanza degli elementi dell'input.

3. **Pesi di Attention**:
    - I pesi normalizzati ottenuti attraverso la funzione softmax rappresentano i pesi di attention effettivi. Ogni peso
      indica quanto peso dare all'elemento dell'input corrispondente quando si produce l'output.
    - I pesi più alti indicano maggiore attenzione a quell'elemento, mentre i pesi più bassi indicano minore attenzione.

4. **Ponderazione dell'Input**:
    - Una volta ottenuti i pesi di attention, essi vengono utilizzati per pesare ciascun elemento dell'input. Questo
      significa moltiplicare ogni elemento dell'input per il rispettivo peso di attention.
    - Gli elementi dell'input con pesi più alti contribuiranno maggiormente all'output finale, mentre quelli con pesi
      più bassi avranno una minore influenza.

In sintesi, il calcolo dei pesi nell'attention coinvolge la valutazione di quanto bene ciascun elemento dell'input si
adatti alle esigenze del compito in corso, seguito dalla normalizzazione dei punteggi attraverso la funzione softmax per
ottenere pesi di attention interpretabili come probabilità. Questi pesi vengono quindi utilizzati per pesare l'input,
influenzando l'output del modello in base all'importanza relativa degli elementi dell'input.

---

## Contextualized embeddings

[~ Vai all'indice](#indice-degli-argomenti)

Gli "embeddings contestualizzati" (contextualized embeddings) sono una forma avanzata di rappresentazioni vettoriali di
parole o frasi utilizzate nell'elaborazione del linguaggio naturale (NLP). A differenza degli embeddings "statici" come
Word2Vec o GloVe, che assegnano un vettore fisso a ciascuna parola indipendentemente dal contesto, gli embeddings
contestualizzati tengono conto del contesto in cui una parola appare in una frase. Ecco come funzionano:

1. **Rete Neurale Ricorrente o Trasformatore**:
    - Per creare embeddings contestualizzati, si utilizza una rete neurale ricorrente (RNN) o un modello Trasformatore,
      come il modello BERT (Bidirectional Encoder Representations from Transformers).
    - Questi modelli sono in grado di elaborare sequenze di parole in modo bidirezionale, ossia considerano sia il
      contesto precedente che quello successivo a una determinata parola nella frase.

2. **Calcolo dell'Embedding**:
    - Per calcolare l'embedding contestualizzato di una parola o di una frase, si passa la sequenza completa alla rete
      neurale. La rete elabora ciascuna parola tenendo conto del contesto circostante, e il vettore risultante è
      specifico per quella parola nella posizione data all'interno della frase.
    - Questo significa che lo stesso termine in posizioni diverse di una frase avrà embedding diversi, poiché il
      contesto è diverso.

3. **Informazioni Contestuali**:
    - Gli embeddings contestualizzati catturano informazioni contestuali. Ad esempio, se la parola "banca" appare in una
      frase su finanza, avrà un embedding diverso rispetto a quando appare in una frase su geografia.
    - Questo aiuta i modelli NLP a comprendere il significato delle parole in base al contesto in cui sono utilizzate,
      migliorando le prestazioni in compiti come la classificazione di sentimenti, il riconoscimento dell'entità
      nominata, la traduzione automatica e altri.

4. **Applicazioni**:
    - Gli embeddings contestualizzati sono diventati popolari per una vasta gamma di applicazioni NLP. Ad esempio, BERT
      è noto per i suoi risultati eccezionali in molti compiti, spaziando dalla comprensione del linguaggio naturale
      alle domande e risposte, alla generazione di testi.

5. **Pre-Addestramento e Fine-Tuning**:
    - Solitamente, i modelli di embedding contestualizzato vengono pre-addestrati su grandi corpus di testi per
      catturare una vasta gamma di conoscenze linguistiche. Successivamente, possono essere sottoposti a "fine-tuning"
      su dataset specifici per adattarli a compiti particolari.

In sintesi, gli embeddings contestualizzati sono rappresentazioni vettoriali delle parole o delle frasi che tengono
conto del contesto in cui si trovano. Questo approccio ha dimostrato di essere estremamente efficace nell'elaborazione
del linguaggio naturale poiché cattura relazioni e significati più ricchi delle parole, contribuendo a migliorare le
prestazioni dei modelli NLP in una varietà di compiti.

---

## WordPiece tokenization

[~ Vai all'indice](#indice-degli-argomenti)

La tokenizzazione Word Piece (WordPiece Tokenization) è una tecnica di suddivisione di testi in unità più piccole,
chiamate "pezzi" (pieces), utilizzata nell'ambito del Natural Language Processing (NLP). Questa tecnica è stata
introdotta per gestire la flessibilità e la ricchezza delle lingue in modo più efficace, consentendo di rappresentare
una vasta gamma di parole e costruzioni linguistiche.

Ecco come funziona la tokenizzazione Word Piece:

1. **Creazione del Vocabolario**:
    - Innanzitutto, si crea un vocabolario di base che contiene le unità più comuni della lingua, come le parole.
      Tuttavia, il vocabolario di base potrebbe non coprire tutte le parole o le varianti linguistiche, specialmente in
      lingue ricche di forme flesse e derivazioni.

2. **Divisione in Pezzi**:
    - La tokenizzazione Word Piece cerca di suddividere le parole in pezzi più piccoli, mantenendo pezzi del vocabolario
      di base quando possibile e dividendo il resto in pezzi più piccoli, come sottoinsiemi di caratteri.
    - Ad esempio, la parola "unfriendly" potrebbe essere suddivisa in "un", "friend", e "ly". Qui, "un" e "ly" sono
      parti del vocabolario di base, mentre "friend" è stato suddiviso in "friend" e "ly".

3. **Codifica del Testo**:
    - Ogni parola nel testo viene sostituita con i suoi pezzi corrispondenti. Quindi, "unfriendly" verrebbe codificato
      come "un friend ly" usando i pezzi estratti.
    - Questo processo rende il testo più flessibile, in quanto può rappresentare una vasta gamma di parole e costruzioni
      linguistiche utilizzando i pezzi più piccoli.

4. **Addestramento del Modello NLP**:
    - Questo tipo di tokenizzazione è spesso utilizzato in combinazione con modelli NLP avanzati, come i modelli di
      linguaggio basati su trasformatori come BERT. Questi modelli possono gestire il testo tokenizzato con pezzi e
      apprendere relazioni complesse tra i pezzi per compiti NLP.

5. **Benefici**:
    - La tokenizzazione Word Piece consente di affrontare problemi di lingue con vocabolari estesi, lingue agglutinanti
      o lingue con strutture morfologiche complesse.
    - Inoltre, riduce la dimensione del vocabolario rispetto a un approccio tradizionale basato su parole, rendendo i
      modelli più gestibili e più efficaci nell'elaborazione del testo.

In sintesi, la tokenizzazione Word Piece è una tecnica di suddivisione del testo che spezza le parole in unità più
piccole, consentendo di gestire meglio la flessibilità e la ricchezza delle lingue nelle applicazioni di elaborazione
del linguaggio naturale. Questa tecnica è particolarmente utile quando si lavora con lingue complesse o con una vasta
gamma di forme linguistiche.

---

## MLM: Masked Language Model

[~ Vai all'indice](#indice-degli-argomenti)

La Masked Language Model (MLM) è un tipo di modello di linguaggio utilizzato nell'ambito dell'elaborazione del
linguaggio naturale (NLP). Questo tipo di modello è stato introdotto con successo in modelli come BERT (Bidirectional
Encoder Representations from Transformers). La caratteristica distintiva di un MLM è la sua capacità di prevedere parole
mancanti o "mascherate" all'interno di una frase o di un testo. Ecco come funziona:

1. **Mascheramento delle Parole**:
    - Durante la fase di pre-addestramento, una parte del testo di addestramento viene selezionata casualmente e le sue
      parole vengono mascherate. Ciò significa che alcune parole vengono nascoste o sostituite con un token speciale di
      maschera (spesso indicato come [MASK]).

2. **Obiettivo di Predizione**:
    - L'obiettivo principale del MLM è far sì che il modello impari a prevedere correttamente le parole mascherate. Il
      modello riceve in input la frase con alcune parole mascherate e deve generare una distribuzione di probabilità su
      tutte le parole possibili per ciascuna posizione mascherata.
    - Ad esempio, se la frase è "Il gatto [MASK] sul [MASK]", il modello dovrebbe prevedere correttamente le parole
      mancanti, come "corre" e "tetto".

3. **Apprendimento Contestualizzato**:
    - Ciò che rende il MLM potente è che il modello è addestrato in modo contestualizzato. Questo significa che tiene
      conto del contesto delle parole circostanti quando prevede le parole mascherate. Ad esempio, nel contesto "Il
      gatto [MASK] sul [MASK]", la parola mancante dopo "[MASK]" potrebbe essere influenzata dal fatto che il gatto è
      solito correre sui tetti.

4. **Addestramento Bidirezionale**:
    - Una caratteristica importante del MLM è che è addestrato in modo bidirezionale, il che significa che può vedere
      sia le parole a sinistra che a destra delle parole mascherate. Ciò permette al modello di comprendere meglio il
      contesto in cui le parole sono utilizzate.

5. **Benefici**:
    - I modelli basati su MLM come BERT hanno dimostrato di ottenere risultati eccezionali in una vasta gamma di compiti
      NLP, poiché riescono a catturare relazioni complesse tra le parole e a comprendere il significato del contesto in
      modo più profondo.

6. **Fine-Tuning**:
    - Dopo la fase di pre-addestramento, i modelli MLM possono essere ulteriormente addestrati su dati specifici per
      compiti NLP particolari attraverso un processo chiamato "fine-tuning". Questo li rende adatti a compiti specifici
      come la classificazione di sentimenti, la traduzione automatica, il riconoscimento di entità nominata e molti
      altri.

In sintesi, la Masked Language Model (MLM) è un tipo di modello di linguaggio che impara a prevedere correttamente le
parole mancanti in una frase o in un testo mascherato durante la fase di addestramento. Questo approccio
contestualizzato e bidirezionale ha dimostrato di essere molto efficace nell'elaborazione del linguaggio naturale,
consentendo ai modelli di comprendere meglio il significato del testo e migliorando le prestazioni in una varietà di
compiti NLP.